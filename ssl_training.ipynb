{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18, resnet34\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/p3i0t/SimCLR-CIFAR10/blob/master/README.md\n",
    "def get_lr(step, total_steps, lr_max, lr_min):\n",
    "    \"\"\"Compute learning rate according to cosine annealing schedule.\"\"\"\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "def get_color_distortion():\n",
    "    color_jitter = transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/p3i0t/SimCLR-CIFAR10/blob/master/README.md\n",
    "class CIFAR10Pair(CIFAR10):\n",
    "    \"\"\"Generate mini-batche pairs on CIFAR10 training set.\"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        imgs = [self.transform(img), self.transform(img)]\n",
    "        return torch.stack(imgs), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRModel(torch.nn.Module):\n",
    "    def __init__(self, backbone, projection_dim=128):\n",
    "        super(SimCLRModel, self).__init__()\n",
    "        self.encoder = backbone(pretrained=False)  \n",
    "\n",
    "        self.encoder.conv1 = torch.nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.encoder.maxpool = torch.nn.Identity()\n",
    "\n",
    "        self.feature_dim = self.encoder.fc.in_features\n",
    "        self.encoder.fc = torch.nn.Identity()\n",
    "\n",
    "        # Projection head\n",
    "        self.projector = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.feature_dim, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        projection = self.projector(features)\n",
    "\n",
    "        return features, projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/p3i0t/SimCLR-CIFAR10/blob/master/README.md\n",
    "def nt_xent(x, t=0.5):\n",
    "    x = F.normalize(x, dim=1)\n",
    "    x_scores =  (x @ x.t()).clamp(min=1e-7)\n",
    "    x_scale = x_scores / t \n",
    "    x_scale = x_scale - torch.eye(x_scale.size(0)).to(x_scale.device) * 1e5\n",
    "\n",
    "    # targets 2N elements.\n",
    "    targets = torch.arange(x.size()[0])\n",
    "    targets[::2] += 1\n",
    "\n",
    "    targets[1::2] -= 1\n",
    "    return F.cross_entropy(x_scale, targets.long().to(x_scale.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(32),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                          get_color_distortion(),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_set = CIFAR10Pair(root='./data', train=True, transform=train_transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 5.7563\n",
      "Epoch [2/1000], Training Loss: 5.5548\n",
      "Epoch [3/1000], Training Loss: 5.4146\n",
      "Epoch [4/1000], Training Loss: 5.3315\n",
      "Epoch [5/1000], Training Loss: 5.2597\n",
      "Epoch [6/1000], Training Loss: 5.2082\n",
      "Epoch [7/1000], Training Loss: 5.1680\n",
      "Epoch [8/1000], Training Loss: 5.1393\n",
      "Epoch [9/1000], Training Loss: 5.1127\n",
      "Epoch [10/1000], Training Loss: 5.0921\n",
      "Epoch [11/1000], Training Loss: 5.0691\n",
      "Epoch [12/1000], Training Loss: 5.0533\n",
      "Epoch [13/1000], Training Loss: 5.0364\n",
      "Epoch [14/1000], Training Loss: 5.0247\n",
      "Epoch [15/1000], Training Loss: 5.0103\n",
      "Epoch [16/1000], Training Loss: 4.9947\n",
      "Epoch [17/1000], Training Loss: 4.9897\n",
      "Epoch [18/1000], Training Loss: 4.9817\n",
      "Epoch [19/1000], Training Loss: 4.9724\n",
      "Epoch [20/1000], Training Loss: 4.9678\n",
      "Epoch [21/1000], Training Loss: 4.9571\n",
      "Epoch [22/1000], Training Loss: 4.9528\n",
      "Epoch [23/1000], Training Loss: 4.9441\n",
      "Epoch [24/1000], Training Loss: 4.9382\n",
      "Epoch [25/1000], Training Loss: 4.9311\n",
      "Epoch [26/1000], Training Loss: 4.9259\n",
      "Epoch [27/1000], Training Loss: 4.9235\n",
      "Epoch [28/1000], Training Loss: 4.9168\n",
      "Epoch [29/1000], Training Loss: 4.9182\n",
      "Epoch [30/1000], Training Loss: 4.9090\n",
      "Epoch [31/1000], Training Loss: 4.9086\n",
      "Epoch [32/1000], Training Loss: 4.9028\n",
      "Epoch [33/1000], Training Loss: 4.8972\n",
      "Epoch [34/1000], Training Loss: 4.8970\n",
      "Epoch [35/1000], Training Loss: 4.8925\n",
      "Epoch [36/1000], Training Loss: 4.8904\n",
      "Epoch [37/1000], Training Loss: 4.8858\n",
      "Epoch [38/1000], Training Loss: 4.8835\n",
      "Epoch [39/1000], Training Loss: 4.8821\n",
      "Epoch [40/1000], Training Loss: 4.8786\n",
      "Epoch [41/1000], Training Loss: 4.8750\n",
      "Epoch [42/1000], Training Loss: 4.8747\n",
      "Epoch [43/1000], Training Loss: 4.8699\n",
      "Epoch [44/1000], Training Loss: 4.8700\n",
      "Epoch [45/1000], Training Loss: 4.8661\n",
      "Epoch [46/1000], Training Loss: 4.8663\n",
      "Epoch [47/1000], Training Loss: 4.8634\n",
      "Epoch [48/1000], Training Loss: 4.8579\n",
      "Epoch [49/1000], Training Loss: 4.8559\n",
      "Epoch [50/1000], Training Loss: 4.8562\n",
      "Epoch [51/1000], Training Loss: 4.8540\n",
      "Epoch [52/1000], Training Loss: 4.8516\n",
      "Epoch [53/1000], Training Loss: 4.8522\n",
      "Epoch [54/1000], Training Loss: 4.8497\n",
      "Epoch [55/1000], Training Loss: 4.8478\n",
      "Epoch [56/1000], Training Loss: 4.8447\n",
      "Epoch [57/1000], Training Loss: 4.8422\n",
      "Epoch [58/1000], Training Loss: 4.8403\n",
      "Epoch [59/1000], Training Loss: 4.8403\n",
      "Epoch [60/1000], Training Loss: 4.8374\n",
      "Epoch [61/1000], Training Loss: 4.8342\n",
      "Epoch [62/1000], Training Loss: 4.8368\n",
      "Epoch [63/1000], Training Loss: 4.8363\n",
      "Epoch [64/1000], Training Loss: 4.8311\n",
      "Epoch [65/1000], Training Loss: 4.8271\n",
      "Epoch [66/1000], Training Loss: 4.8301\n",
      "Epoch [67/1000], Training Loss: 4.8272\n",
      "Epoch [68/1000], Training Loss: 4.8243\n",
      "Epoch [69/1000], Training Loss: 4.8248\n",
      "Epoch [70/1000], Training Loss: 4.8258\n",
      "Epoch [71/1000], Training Loss: 4.8235\n",
      "Epoch [72/1000], Training Loss: 4.8197\n",
      "Epoch [73/1000], Training Loss: 4.8246\n",
      "Epoch [74/1000], Training Loss: 4.8169\n",
      "Epoch [75/1000], Training Loss: 4.8174\n",
      "Epoch [76/1000], Training Loss: 4.8185\n",
      "Epoch [77/1000], Training Loss: 4.8114\n",
      "Epoch [78/1000], Training Loss: 4.8158\n",
      "Epoch [79/1000], Training Loss: 4.8143\n",
      "Epoch [80/1000], Training Loss: 4.8120\n",
      "Epoch [81/1000], Training Loss: 4.8117\n",
      "Epoch [82/1000], Training Loss: 4.8109\n",
      "Epoch [83/1000], Training Loss: 4.8085\n",
      "Epoch [84/1000], Training Loss: 4.8051\n",
      "Epoch [85/1000], Training Loss: 4.8032\n",
      "Epoch [86/1000], Training Loss: 4.8049\n",
      "Epoch [87/1000], Training Loss: 4.8067\n",
      "Epoch [88/1000], Training Loss: 4.8034\n",
      "Epoch [89/1000], Training Loss: 4.8011\n",
      "Epoch [90/1000], Training Loss: 4.7963\n",
      "Epoch [91/1000], Training Loss: 4.7998\n",
      "Epoch [92/1000], Training Loss: 4.7965\n",
      "Epoch [93/1000], Training Loss: 4.7965\n",
      "Epoch [94/1000], Training Loss: 4.7969\n",
      "Epoch [95/1000], Training Loss: 4.7925\n",
      "Epoch [96/1000], Training Loss: 4.7957\n",
      "Epoch [97/1000], Training Loss: 4.7936\n",
      "Epoch [98/1000], Training Loss: 4.7905\n",
      "Epoch [99/1000], Training Loss: 4.7920\n",
      "Epoch [100/1000], Training Loss: 4.7929\n",
      "Epoch [101/1000], Training Loss: 4.7882\n",
      "Epoch [102/1000], Training Loss: 4.7900\n",
      "Epoch [103/1000], Training Loss: 4.7905\n",
      "Epoch [104/1000], Training Loss: 4.7899\n",
      "Epoch [105/1000], Training Loss: 4.7865\n",
      "Epoch [106/1000], Training Loss: 4.7887\n",
      "Epoch [107/1000], Training Loss: 4.7851\n",
      "Epoch [108/1000], Training Loss: 4.7834\n",
      "Epoch [109/1000], Training Loss: 4.7846\n",
      "Epoch [110/1000], Training Loss: 4.7824\n",
      "Epoch [111/1000], Training Loss: 4.7832\n",
      "Epoch [112/1000], Training Loss: 4.7807\n",
      "Epoch [113/1000], Training Loss: 4.7805\n",
      "Epoch [114/1000], Training Loss: 4.7834\n",
      "Epoch [115/1000], Training Loss: 4.7796\n",
      "Epoch [116/1000], Training Loss: 4.7819\n",
      "Epoch [117/1000], Training Loss: 4.7770\n",
      "Epoch [118/1000], Training Loss: 4.7782\n",
      "Epoch [119/1000], Training Loss: 4.7773\n",
      "Epoch [120/1000], Training Loss: 4.7789\n",
      "Epoch [121/1000], Training Loss: 4.7770\n",
      "Epoch [122/1000], Training Loss: 4.7756\n",
      "Epoch [123/1000], Training Loss: 4.7706\n",
      "Epoch [124/1000], Training Loss: 4.7728\n",
      "Epoch [125/1000], Training Loss: 4.7737\n",
      "Epoch [126/1000], Training Loss: 4.7729\n",
      "Epoch [127/1000], Training Loss: 4.7696\n",
      "Epoch [128/1000], Training Loss: 4.7717\n",
      "Epoch [129/1000], Training Loss: 4.7722\n",
      "Epoch [130/1000], Training Loss: 4.7691\n",
      "Epoch [131/1000], Training Loss: 4.7713\n",
      "Epoch [132/1000], Training Loss: 4.7659\n",
      "Epoch [133/1000], Training Loss: 4.7654\n",
      "Epoch [134/1000], Training Loss: 4.7702\n",
      "Epoch [135/1000], Training Loss: 4.7668\n",
      "Epoch [136/1000], Training Loss: 4.7627\n",
      "Epoch [137/1000], Training Loss: 4.7630\n",
      "Epoch [138/1000], Training Loss: 4.7636\n",
      "Epoch [139/1000], Training Loss: 4.7615\n",
      "Epoch [140/1000], Training Loss: 4.7678\n",
      "Epoch [141/1000], Training Loss: 4.7621\n",
      "Epoch [142/1000], Training Loss: 4.7611\n",
      "Epoch [143/1000], Training Loss: 4.7641\n",
      "Epoch [144/1000], Training Loss: 4.7607\n",
      "Epoch [145/1000], Training Loss: 4.7602\n",
      "Epoch [146/1000], Training Loss: 4.7564\n",
      "Epoch [147/1000], Training Loss: 4.7600\n",
      "Epoch [148/1000], Training Loss: 4.7606\n",
      "Epoch [149/1000], Training Loss: 4.7576\n",
      "Epoch [150/1000], Training Loss: 4.7614\n",
      "Epoch [151/1000], Training Loss: 4.7553\n",
      "Epoch [152/1000], Training Loss: 4.7599\n",
      "Epoch [153/1000], Training Loss: 4.7588\n",
      "Epoch [154/1000], Training Loss: 4.7546\n",
      "Epoch [155/1000], Training Loss: 4.7552\n",
      "Epoch [156/1000], Training Loss: 4.7551\n",
      "Epoch [157/1000], Training Loss: 4.7507\n",
      "Epoch [158/1000], Training Loss: 4.7542\n",
      "Epoch [159/1000], Training Loss: 4.7490\n",
      "Epoch [160/1000], Training Loss: 4.7535\n",
      "Epoch [161/1000], Training Loss: 4.7520\n",
      "Epoch [162/1000], Training Loss: 4.7521\n",
      "Epoch [163/1000], Training Loss: 4.7510\n",
      "Epoch [164/1000], Training Loss: 4.7488\n",
      "Epoch [165/1000], Training Loss: 4.7449\n",
      "Epoch [166/1000], Training Loss: 4.7482\n",
      "Epoch [167/1000], Training Loss: 4.7505\n",
      "Epoch [168/1000], Training Loss: 4.7478\n",
      "Epoch [169/1000], Training Loss: 4.7477\n",
      "Epoch [170/1000], Training Loss: 4.7481\n",
      "Epoch [171/1000], Training Loss: 4.7473\n",
      "Epoch [172/1000], Training Loss: 4.7419\n",
      "Epoch [173/1000], Training Loss: 4.7452\n",
      "Epoch [174/1000], Training Loss: 4.7446\n",
      "Epoch [175/1000], Training Loss: 4.7460\n",
      "Epoch [176/1000], Training Loss: 4.7428\n",
      "Epoch [177/1000], Training Loss: 4.7417\n",
      "Epoch [178/1000], Training Loss: 4.7438\n",
      "Epoch [179/1000], Training Loss: 4.7443\n",
      "Epoch [180/1000], Training Loss: 4.7382\n",
      "Epoch [181/1000], Training Loss: 4.7420\n",
      "Epoch [182/1000], Training Loss: 4.7415\n",
      "Epoch [183/1000], Training Loss: 4.7414\n",
      "Epoch [184/1000], Training Loss: 4.7402\n",
      "Epoch [185/1000], Training Loss: 4.7390\n",
      "Epoch [186/1000], Training Loss: 4.7382\n",
      "Epoch [187/1000], Training Loss: 4.7376\n",
      "Epoch [188/1000], Training Loss: 4.7374\n",
      "Epoch [189/1000], Training Loss: 4.7377\n",
      "Epoch [190/1000], Training Loss: 4.7363\n",
      "Epoch [191/1000], Training Loss: 4.7365\n",
      "Epoch [192/1000], Training Loss: 4.7348\n",
      "Epoch [193/1000], Training Loss: 4.7348\n",
      "Epoch [194/1000], Training Loss: 4.7391\n",
      "Epoch [195/1000], Training Loss: 4.7350\n",
      "Epoch [196/1000], Training Loss: 4.7349\n",
      "Epoch [197/1000], Training Loss: 4.7335\n",
      "Epoch [198/1000], Training Loss: 4.7302\n",
      "Epoch [199/1000], Training Loss: 4.7334\n",
      "Epoch [200/1000], Training Loss: 4.7316\n",
      "Epoch [201/1000], Training Loss: 4.7320\n",
      "Epoch [202/1000], Training Loss: 4.7330\n",
      "Epoch [203/1000], Training Loss: 4.7336\n",
      "Epoch [204/1000], Training Loss: 4.7311\n",
      "Epoch [205/1000], Training Loss: 4.7310\n",
      "Epoch [206/1000], Training Loss: 4.7275\n",
      "Epoch [207/1000], Training Loss: 4.7302\n",
      "Epoch [208/1000], Training Loss: 4.7298\n",
      "Epoch [209/1000], Training Loss: 4.7284\n",
      "Epoch [210/1000], Training Loss: 4.7247\n",
      "Epoch [211/1000], Training Loss: 4.7258\n",
      "Epoch [212/1000], Training Loss: 4.7268\n",
      "Epoch [213/1000], Training Loss: 4.7270\n",
      "Epoch [214/1000], Training Loss: 4.7263\n",
      "Epoch [215/1000], Training Loss: 4.7279\n",
      "Epoch [216/1000], Training Loss: 4.7257\n",
      "Epoch [217/1000], Training Loss: 4.7284\n",
      "Epoch [218/1000], Training Loss: 4.7249\n",
      "Epoch [219/1000], Training Loss: 4.7228\n",
      "Epoch [220/1000], Training Loss: 4.7253\n",
      "Epoch [221/1000], Training Loss: 4.7233\n",
      "Epoch [222/1000], Training Loss: 4.7255\n",
      "Epoch [223/1000], Training Loss: 4.7204\n",
      "Epoch [224/1000], Training Loss: 4.7187\n",
      "Epoch [225/1000], Training Loss: 4.7232\n",
      "Epoch [226/1000], Training Loss: 4.7232\n",
      "Epoch [227/1000], Training Loss: 4.7246\n",
      "Epoch [228/1000], Training Loss: 4.7211\n",
      "Epoch [229/1000], Training Loss: 4.7184\n",
      "Epoch [230/1000], Training Loss: 4.7189\n",
      "Epoch [231/1000], Training Loss: 4.7201\n",
      "Epoch [232/1000], Training Loss: 4.7207\n",
      "Epoch [233/1000], Training Loss: 4.7206\n",
      "Epoch [234/1000], Training Loss: 4.7205\n",
      "Epoch [235/1000], Training Loss: 4.7194\n",
      "Epoch [236/1000], Training Loss: 4.7165\n",
      "Epoch [237/1000], Training Loss: 4.7163\n",
      "Epoch [238/1000], Training Loss: 4.7165\n",
      "Epoch [239/1000], Training Loss: 4.7180\n",
      "Epoch [240/1000], Training Loss: 4.7180\n",
      "Epoch [241/1000], Training Loss: 4.7176\n",
      "Epoch [242/1000], Training Loss: 4.7169\n",
      "Epoch [243/1000], Training Loss: 4.7157\n",
      "Epoch [244/1000], Training Loss: 4.7164\n",
      "Epoch [245/1000], Training Loss: 4.7129\n",
      "Epoch [246/1000], Training Loss: 4.7147\n",
      "Epoch [247/1000], Training Loss: 4.7131\n",
      "Epoch [248/1000], Training Loss: 4.7162\n",
      "Epoch [249/1000], Training Loss: 4.7127\n",
      "Epoch [250/1000], Training Loss: 4.7127\n",
      "Epoch [251/1000], Training Loss: 4.7135\n",
      "Epoch [252/1000], Training Loss: 4.7116\n",
      "Epoch [253/1000], Training Loss: 4.7142\n",
      "Epoch [254/1000], Training Loss: 4.7123\n",
      "Epoch [255/1000], Training Loss: 4.7117\n",
      "Epoch [256/1000], Training Loss: 4.7092\n",
      "Epoch [257/1000], Training Loss: 4.7150\n",
      "Epoch [258/1000], Training Loss: 4.7095\n",
      "Epoch [259/1000], Training Loss: 4.7122\n",
      "Epoch [260/1000], Training Loss: 4.7145\n",
      "Epoch [261/1000], Training Loss: 4.7084\n",
      "Epoch [262/1000], Training Loss: 4.7082\n",
      "Epoch [263/1000], Training Loss: 4.7100\n",
      "Epoch [264/1000], Training Loss: 4.7066\n",
      "Epoch [265/1000], Training Loss: 4.7133\n",
      "Epoch [266/1000], Training Loss: 4.7107\n",
      "Epoch [267/1000], Training Loss: 4.7113\n",
      "Epoch [268/1000], Training Loss: 4.7098\n",
      "Epoch [269/1000], Training Loss: 4.7080\n",
      "Epoch [270/1000], Training Loss: 4.7088\n",
      "Epoch [271/1000], Training Loss: 4.7055\n",
      "Epoch [272/1000], Training Loss: 4.7043\n",
      "Epoch [273/1000], Training Loss: 4.7082\n",
      "Epoch [274/1000], Training Loss: 4.7069\n",
      "Epoch [275/1000], Training Loss: 4.7070\n",
      "Epoch [276/1000], Training Loss: 4.7038\n",
      "Epoch [277/1000], Training Loss: 4.7033\n",
      "Epoch [278/1000], Training Loss: 4.7037\n",
      "Epoch [279/1000], Training Loss: 4.7054\n",
      "Epoch [280/1000], Training Loss: 4.7054\n",
      "Epoch [281/1000], Training Loss: 4.7012\n",
      "Epoch [282/1000], Training Loss: 4.7041\n",
      "Epoch [283/1000], Training Loss: 4.7038\n",
      "Epoch [284/1000], Training Loss: 4.7041\n",
      "Epoch [285/1000], Training Loss: 4.7018\n",
      "Epoch [286/1000], Training Loss: 4.7040\n",
      "Epoch [287/1000], Training Loss: 4.7023\n",
      "Epoch [288/1000], Training Loss: 4.7033\n",
      "Epoch [289/1000], Training Loss: 4.7017\n",
      "Epoch [290/1000], Training Loss: 4.6996\n",
      "Epoch [291/1000], Training Loss: 4.7010\n",
      "Epoch [292/1000], Training Loss: 4.7042\n",
      "Epoch [293/1000], Training Loss: 4.6988\n",
      "Epoch [294/1000], Training Loss: 4.7019\n",
      "Epoch [295/1000], Training Loss: 4.6981\n",
      "Epoch [296/1000], Training Loss: 4.7000\n",
      "Epoch [297/1000], Training Loss: 4.6977\n",
      "Epoch [298/1000], Training Loss: 4.6949\n",
      "Epoch [299/1000], Training Loss: 4.7014\n",
      "Epoch [300/1000], Training Loss: 4.7019\n",
      "Epoch [301/1000], Training Loss: 4.6975\n",
      "Epoch [302/1000], Training Loss: 4.6967\n",
      "Epoch [303/1000], Training Loss: 4.6953\n",
      "Epoch [304/1000], Training Loss: 4.6970\n",
      "Epoch [305/1000], Training Loss: 4.6939\n",
      "Epoch [306/1000], Training Loss: 4.6966\n",
      "Epoch [307/1000], Training Loss: 4.6962\n",
      "Epoch [308/1000], Training Loss: 4.6915\n",
      "Epoch [309/1000], Training Loss: 4.6959\n",
      "Epoch [310/1000], Training Loss: 4.6962\n",
      "Epoch [311/1000], Training Loss: 4.6934\n",
      "Epoch [312/1000], Training Loss: 4.6960\n",
      "Epoch [313/1000], Training Loss: 4.6944\n",
      "Epoch [314/1000], Training Loss: 4.6933\n",
      "Epoch [315/1000], Training Loss: 4.6959\n",
      "Epoch [316/1000], Training Loss: 4.6947\n",
      "Epoch [317/1000], Training Loss: 4.6924\n",
      "Epoch [318/1000], Training Loss: 4.6932\n",
      "Epoch [319/1000], Training Loss: 4.6942\n",
      "Epoch [320/1000], Training Loss: 4.6919\n",
      "Epoch [321/1000], Training Loss: 4.6929\n",
      "Epoch [322/1000], Training Loss: 4.6912\n",
      "Epoch [323/1000], Training Loss: 4.6867\n",
      "Epoch [324/1000], Training Loss: 4.6896\n",
      "Epoch [325/1000], Training Loss: 4.6888\n",
      "Epoch [326/1000], Training Loss: 4.6872\n",
      "Epoch [327/1000], Training Loss: 4.6886\n",
      "Epoch [328/1000], Training Loss: 4.6899\n",
      "Epoch [329/1000], Training Loss: 4.6918\n",
      "Epoch [330/1000], Training Loss: 4.6893\n",
      "Epoch [331/1000], Training Loss: 4.6896\n",
      "Epoch [332/1000], Training Loss: 4.6916\n",
      "Epoch [333/1000], Training Loss: 4.6862\n",
      "Epoch [334/1000], Training Loss: 4.6843\n",
      "Epoch [335/1000], Training Loss: 4.6886\n",
      "Epoch [336/1000], Training Loss: 4.6865\n",
      "Epoch [337/1000], Training Loss: 4.6837\n",
      "Epoch [338/1000], Training Loss: 4.6888\n",
      "Epoch [339/1000], Training Loss: 4.6871\n",
      "Epoch [340/1000], Training Loss: 4.6895\n",
      "Epoch [341/1000], Training Loss: 4.6876\n",
      "Epoch [342/1000], Training Loss: 4.6872\n",
      "Epoch [343/1000], Training Loss: 4.6854\n",
      "Epoch [344/1000], Training Loss: 4.6852\n",
      "Epoch [345/1000], Training Loss: 4.6844\n",
      "Epoch [346/1000], Training Loss: 4.6863\n",
      "Epoch [347/1000], Training Loss: 4.6834\n",
      "Epoch [348/1000], Training Loss: 4.6846\n",
      "Epoch [349/1000], Training Loss: 4.6847\n",
      "Epoch [350/1000], Training Loss: 4.6862\n",
      "Epoch [351/1000], Training Loss: 4.6792\n",
      "Epoch [352/1000], Training Loss: 4.6822\n",
      "Epoch [353/1000], Training Loss: 4.6855\n",
      "Epoch [354/1000], Training Loss: 4.6858\n",
      "Epoch [355/1000], Training Loss: 4.6843\n",
      "Epoch [356/1000], Training Loss: 4.6837\n",
      "Epoch [357/1000], Training Loss: 4.6834\n",
      "Epoch [358/1000], Training Loss: 4.6824\n",
      "Epoch [359/1000], Training Loss: 4.6839\n",
      "Epoch [360/1000], Training Loss: 4.6805\n",
      "Epoch [361/1000], Training Loss: 4.6821\n",
      "Epoch [362/1000], Training Loss: 4.6836\n",
      "Epoch [363/1000], Training Loss: 4.6834\n",
      "Epoch [364/1000], Training Loss: 4.6809\n",
      "Epoch [365/1000], Training Loss: 4.6786\n",
      "Epoch [366/1000], Training Loss: 4.6812\n",
      "Epoch [367/1000], Training Loss: 4.6786\n",
      "Epoch [368/1000], Training Loss: 4.6783\n",
      "Epoch [369/1000], Training Loss: 4.6790\n",
      "Epoch [370/1000], Training Loss: 4.6788\n",
      "Epoch [371/1000], Training Loss: 4.6786\n",
      "Epoch [372/1000], Training Loss: 4.6747\n",
      "Epoch [373/1000], Training Loss: 4.6805\n",
      "Epoch [374/1000], Training Loss: 4.6803\n",
      "Epoch [375/1000], Training Loss: 4.6787\n",
      "Epoch [376/1000], Training Loss: 4.6831\n",
      "Epoch [377/1000], Training Loss: 4.6771\n",
      "Epoch [378/1000], Training Loss: 4.6761\n",
      "Epoch [379/1000], Training Loss: 4.6773\n",
      "Epoch [380/1000], Training Loss: 4.6733\n",
      "Epoch [381/1000], Training Loss: 4.6790\n",
      "Epoch [382/1000], Training Loss: 4.6770\n",
      "Epoch [383/1000], Training Loss: 4.6770\n",
      "Epoch [384/1000], Training Loss: 4.6768\n",
      "Epoch [385/1000], Training Loss: 4.6773\n",
      "Epoch [386/1000], Training Loss: 4.6735\n",
      "Epoch [387/1000], Training Loss: 4.6742\n",
      "Epoch [388/1000], Training Loss: 4.6705\n",
      "Epoch [389/1000], Training Loss: 4.6754\n",
      "Epoch [390/1000], Training Loss: 4.6741\n",
      "Epoch [391/1000], Training Loss: 4.6717\n",
      "Epoch [392/1000], Training Loss: 4.6740\n",
      "Epoch [393/1000], Training Loss: 4.6739\n",
      "Epoch [394/1000], Training Loss: 4.6732\n",
      "Epoch [395/1000], Training Loss: 4.6722\n",
      "Epoch [396/1000], Training Loss: 4.6754\n",
      "Epoch [397/1000], Training Loss: 4.6737\n",
      "Epoch [398/1000], Training Loss: 4.6729\n",
      "Epoch [399/1000], Training Loss: 4.6758\n",
      "Epoch [400/1000], Training Loss: 4.6723\n",
      "Epoch [401/1000], Training Loss: 4.6700\n",
      "Epoch [402/1000], Training Loss: 4.6701\n",
      "Epoch [403/1000], Training Loss: 4.6699\n",
      "Epoch [404/1000], Training Loss: 4.6722\n",
      "Epoch [405/1000], Training Loss: 4.6717\n",
      "Epoch [406/1000], Training Loss: 4.6730\n",
      "Epoch [407/1000], Training Loss: 4.6730\n",
      "Epoch [408/1000], Training Loss: 4.6720\n",
      "Epoch [409/1000], Training Loss: 4.6704\n",
      "Epoch [410/1000], Training Loss: 4.6691\n",
      "Epoch [411/1000], Training Loss: 4.6715\n",
      "Epoch [412/1000], Training Loss: 4.6665\n",
      "Epoch [413/1000], Training Loss: 4.6726\n",
      "Epoch [414/1000], Training Loss: 4.6700\n",
      "Epoch [415/1000], Training Loss: 4.6688\n",
      "Epoch [416/1000], Training Loss: 4.6681\n",
      "Epoch [417/1000], Training Loss: 4.6654\n",
      "Epoch [418/1000], Training Loss: 4.6653\n",
      "Epoch [419/1000], Training Loss: 4.6672\n",
      "Epoch [420/1000], Training Loss: 4.6655\n",
      "Epoch [421/1000], Training Loss: 4.6656\n",
      "Epoch [422/1000], Training Loss: 4.6651\n",
      "Epoch [423/1000], Training Loss: 4.6673\n",
      "Epoch [424/1000], Training Loss: 4.6666\n",
      "Epoch [425/1000], Training Loss: 4.6652\n",
      "Epoch [426/1000], Training Loss: 4.6701\n",
      "Epoch [427/1000], Training Loss: 4.6693\n",
      "Epoch [428/1000], Training Loss: 4.6708\n",
      "Epoch [429/1000], Training Loss: 4.6640\n",
      "Epoch [430/1000], Training Loss: 4.6689\n",
      "Epoch [431/1000], Training Loss: 4.6642\n",
      "Epoch [432/1000], Training Loss: 4.6641\n",
      "Epoch [433/1000], Training Loss: 4.6684\n",
      "Epoch [434/1000], Training Loss: 4.6649\n",
      "Epoch [435/1000], Training Loss: 4.6675\n",
      "Epoch [436/1000], Training Loss: 4.6680\n",
      "Epoch [437/1000], Training Loss: 4.6643\n",
      "Epoch [438/1000], Training Loss: 4.6650\n",
      "Epoch [439/1000], Training Loss: 4.6598\n",
      "Epoch [440/1000], Training Loss: 4.6638\n",
      "Epoch [441/1000], Training Loss: 4.6647\n",
      "Epoch [442/1000], Training Loss: 4.6622\n",
      "Epoch [443/1000], Training Loss: 4.6616\n",
      "Epoch [444/1000], Training Loss: 4.6614\n",
      "Epoch [445/1000], Training Loss: 4.6614\n",
      "Epoch [446/1000], Training Loss: 4.6637\n",
      "Epoch [447/1000], Training Loss: 4.6611\n",
      "Epoch [448/1000], Training Loss: 4.6667\n",
      "Epoch [449/1000], Training Loss: 4.6635\n",
      "Epoch [450/1000], Training Loss: 4.6628\n",
      "Epoch [451/1000], Training Loss: 4.6612\n",
      "Epoch [452/1000], Training Loss: 4.6590\n",
      "Epoch [453/1000], Training Loss: 4.6581\n",
      "Epoch [454/1000], Training Loss: 4.6580\n",
      "Epoch [455/1000], Training Loss: 4.6580\n",
      "Epoch [456/1000], Training Loss: 4.6585\n",
      "Epoch [457/1000], Training Loss: 4.6560\n",
      "Epoch [458/1000], Training Loss: 4.6590\n",
      "Epoch [459/1000], Training Loss: 4.6603\n",
      "Epoch [460/1000], Training Loss: 4.6550\n",
      "Epoch [461/1000], Training Loss: 4.6580\n",
      "Epoch [462/1000], Training Loss: 4.6582\n",
      "Epoch [463/1000], Training Loss: 4.6602\n",
      "Epoch [464/1000], Training Loss: 4.6582\n",
      "Epoch [465/1000], Training Loss: 4.6586\n",
      "Epoch [466/1000], Training Loss: 4.6562\n",
      "Epoch [467/1000], Training Loss: 4.6586\n",
      "Epoch [468/1000], Training Loss: 4.6574\n",
      "Epoch [469/1000], Training Loss: 4.6588\n",
      "Epoch [470/1000], Training Loss: 4.6548\n",
      "Epoch [471/1000], Training Loss: 4.6543\n",
      "Epoch [472/1000], Training Loss: 4.6560\n",
      "Epoch [473/1000], Training Loss: 4.6557\n",
      "Epoch [474/1000], Training Loss: 4.6573\n",
      "Epoch [475/1000], Training Loss: 4.6602\n",
      "Epoch [476/1000], Training Loss: 4.6571\n",
      "Epoch [477/1000], Training Loss: 4.6538\n",
      "Epoch [478/1000], Training Loss: 4.6531\n",
      "Epoch [479/1000], Training Loss: 4.6565\n",
      "Epoch [480/1000], Training Loss: 4.6510\n",
      "Epoch [481/1000], Training Loss: 4.6553\n",
      "Epoch [482/1000], Training Loss: 4.6522\n",
      "Epoch [483/1000], Training Loss: 4.6545\n",
      "Epoch [484/1000], Training Loss: 4.6521\n",
      "Epoch [485/1000], Training Loss: 4.6555\n",
      "Epoch [486/1000], Training Loss: 4.6525\n",
      "Epoch [487/1000], Training Loss: 4.6528\n",
      "Epoch [488/1000], Training Loss: 4.6532\n",
      "Epoch [489/1000], Training Loss: 4.6542\n",
      "Epoch [490/1000], Training Loss: 4.6595\n",
      "Epoch [491/1000], Training Loss: 4.6536\n",
      "Epoch [492/1000], Training Loss: 4.6510\n",
      "Epoch [493/1000], Training Loss: 4.6518\n",
      "Epoch [494/1000], Training Loss: 4.6534\n",
      "Epoch [495/1000], Training Loss: 4.6528\n",
      "Epoch [496/1000], Training Loss: 4.6510\n",
      "Epoch [497/1000], Training Loss: 4.6523\n",
      "Epoch [498/1000], Training Loss: 4.6536\n",
      "Epoch [499/1000], Training Loss: 4.6510\n",
      "Epoch [500/1000], Training Loss: 4.6475\n",
      "Epoch [501/1000], Training Loss: 4.6497\n",
      "Epoch [502/1000], Training Loss: 4.6477\n",
      "Epoch [503/1000], Training Loss: 4.6525\n",
      "Epoch [504/1000], Training Loss: 4.6490\n",
      "Epoch [505/1000], Training Loss: 4.6498\n",
      "Epoch [506/1000], Training Loss: 4.6476\n",
      "Epoch [507/1000], Training Loss: 4.6509\n",
      "Epoch [508/1000], Training Loss: 4.6483\n",
      "Epoch [509/1000], Training Loss: 4.6470\n",
      "Epoch [510/1000], Training Loss: 4.6468\n",
      "Epoch [511/1000], Training Loss: 4.6486\n",
      "Epoch [512/1000], Training Loss: 4.6504\n",
      "Epoch [513/1000], Training Loss: 4.6488\n",
      "Epoch [514/1000], Training Loss: 4.6469\n",
      "Epoch [515/1000], Training Loss: 4.6475\n",
      "Epoch [516/1000], Training Loss: 4.6483\n",
      "Epoch [517/1000], Training Loss: 4.6466\n",
      "Epoch [518/1000], Training Loss: 4.6459\n",
      "Epoch [519/1000], Training Loss: 4.6449\n",
      "Epoch [520/1000], Training Loss: 4.6458\n",
      "Epoch [521/1000], Training Loss: 4.6461\n",
      "Epoch [522/1000], Training Loss: 4.6475\n",
      "Epoch [523/1000], Training Loss: 4.6439\n",
      "Epoch [524/1000], Training Loss: 4.6449\n",
      "Epoch [525/1000], Training Loss: 4.6458\n",
      "Epoch [526/1000], Training Loss: 4.6458\n",
      "Epoch [527/1000], Training Loss: 4.6443\n",
      "Epoch [528/1000], Training Loss: 4.6449\n",
      "Epoch [529/1000], Training Loss: 4.6457\n",
      "Epoch [530/1000], Training Loss: 4.6471\n",
      "Epoch [531/1000], Training Loss: 4.6474\n",
      "Epoch [532/1000], Training Loss: 4.6434\n",
      "Epoch [533/1000], Training Loss: 4.6440\n",
      "Epoch [534/1000], Training Loss: 4.6468\n",
      "Epoch [535/1000], Training Loss: 4.6478\n",
      "Epoch [536/1000], Training Loss: 4.6445\n",
      "Epoch [537/1000], Training Loss: 4.6425\n",
      "Epoch [538/1000], Training Loss: 4.6409\n",
      "Epoch [539/1000], Training Loss: 4.6420\n",
      "Epoch [540/1000], Training Loss: 4.6436\n",
      "Epoch [541/1000], Training Loss: 4.6426\n",
      "Epoch [542/1000], Training Loss: 4.6436\n",
      "Epoch [543/1000], Training Loss: 4.6377\n",
      "Epoch [544/1000], Training Loss: 4.6387\n",
      "Epoch [545/1000], Training Loss: 4.6414\n",
      "Epoch [546/1000], Training Loss: 4.6393\n",
      "Epoch [547/1000], Training Loss: 4.6406\n",
      "Epoch [548/1000], Training Loss: 4.6389\n",
      "Epoch [549/1000], Training Loss: 4.6424\n",
      "Epoch [550/1000], Training Loss: 4.6421\n",
      "Epoch [551/1000], Training Loss: 4.6377\n",
      "Epoch [552/1000], Training Loss: 4.6410\n",
      "Epoch [553/1000], Training Loss: 4.6413\n",
      "Epoch [554/1000], Training Loss: 4.6368\n",
      "Epoch [555/1000], Training Loss: 4.6408\n",
      "Epoch [556/1000], Training Loss: 4.6390\n",
      "Epoch [557/1000], Training Loss: 4.6405\n",
      "Epoch [558/1000], Training Loss: 4.6408\n",
      "Epoch [559/1000], Training Loss: 4.6367\n",
      "Epoch [560/1000], Training Loss: 4.6381\n",
      "Epoch [561/1000], Training Loss: 4.6376\n",
      "Epoch [562/1000], Training Loss: 4.6377\n",
      "Epoch [563/1000], Training Loss: 4.6367\n",
      "Epoch [564/1000], Training Loss: 4.6403\n",
      "Epoch [565/1000], Training Loss: 4.6387\n",
      "Epoch [566/1000], Training Loss: 4.6358\n",
      "Epoch [567/1000], Training Loss: 4.6371\n",
      "Epoch [568/1000], Training Loss: 4.6370\n",
      "Epoch [569/1000], Training Loss: 4.6377\n",
      "Epoch [570/1000], Training Loss: 4.6392\n",
      "Epoch [571/1000], Training Loss: 4.6376\n",
      "Epoch [572/1000], Training Loss: 4.6360\n",
      "Epoch [573/1000], Training Loss: 4.6380\n",
      "Epoch [574/1000], Training Loss: 4.6355\n",
      "Epoch [575/1000], Training Loss: 4.6346\n",
      "Epoch [576/1000], Training Loss: 4.6374\n",
      "Epoch [577/1000], Training Loss: 4.6336\n",
      "Epoch [578/1000], Training Loss: 4.6365\n",
      "Epoch [579/1000], Training Loss: 4.6345\n",
      "Epoch [580/1000], Training Loss: 4.6312\n",
      "Epoch [581/1000], Training Loss: 4.6337\n",
      "Epoch [582/1000], Training Loss: 4.6345\n",
      "Epoch [583/1000], Training Loss: 4.6328\n",
      "Epoch [584/1000], Training Loss: 4.6348\n",
      "Epoch [585/1000], Training Loss: 4.6330\n",
      "Epoch [586/1000], Training Loss: 4.6341\n",
      "Epoch [587/1000], Training Loss: 4.6332\n",
      "Epoch [588/1000], Training Loss: 4.6350\n",
      "Epoch [589/1000], Training Loss: 4.6323\n",
      "Epoch [590/1000], Training Loss: 4.6316\n",
      "Epoch [591/1000], Training Loss: 4.6329\n",
      "Epoch [592/1000], Training Loss: 4.6357\n",
      "Epoch [593/1000], Training Loss: 4.6339\n",
      "Epoch [594/1000], Training Loss: 4.6322\n",
      "Epoch [595/1000], Training Loss: 4.6321\n",
      "Epoch [596/1000], Training Loss: 4.6306\n",
      "Epoch [597/1000], Training Loss: 4.6340\n",
      "Epoch [598/1000], Training Loss: 4.6345\n",
      "Epoch [599/1000], Training Loss: 4.6275\n",
      "Epoch [600/1000], Training Loss: 4.6311\n",
      "Epoch [601/1000], Training Loss: 4.6318\n",
      "Epoch [602/1000], Training Loss: 4.6332\n",
      "Epoch [603/1000], Training Loss: 4.6298\n",
      "Epoch [604/1000], Training Loss: 4.6291\n",
      "Epoch [605/1000], Training Loss: 4.6285\n",
      "Epoch [606/1000], Training Loss: 4.6315\n",
      "Epoch [607/1000], Training Loss: 4.6321\n",
      "Epoch [608/1000], Training Loss: 4.6291\n",
      "Epoch [609/1000], Training Loss: 4.6302\n",
      "Epoch [610/1000], Training Loss: 4.6282\n",
      "Epoch [611/1000], Training Loss: 4.6286\n",
      "Epoch [612/1000], Training Loss: 4.6325\n",
      "Epoch [613/1000], Training Loss: 4.6295\n",
      "Epoch [614/1000], Training Loss: 4.6301\n",
      "Epoch [615/1000], Training Loss: 4.6290\n",
      "Epoch [616/1000], Training Loss: 4.6250\n",
      "Epoch [617/1000], Training Loss: 4.6271\n",
      "Epoch [618/1000], Training Loss: 4.6241\n",
      "Epoch [619/1000], Training Loss: 4.6277\n",
      "Epoch [620/1000], Training Loss: 4.6277\n",
      "Epoch [621/1000], Training Loss: 4.6292\n",
      "Epoch [622/1000], Training Loss: 4.6266\n",
      "Epoch [623/1000], Training Loss: 4.6285\n",
      "Epoch [624/1000], Training Loss: 4.6292\n",
      "Epoch [625/1000], Training Loss: 4.6276\n",
      "Epoch [626/1000], Training Loss: 4.6247\n",
      "Epoch [627/1000], Training Loss: 4.6244\n",
      "Epoch [628/1000], Training Loss: 4.6273\n",
      "Epoch [629/1000], Training Loss: 4.6256\n",
      "Epoch [630/1000], Training Loss: 4.6231\n",
      "Epoch [631/1000], Training Loss: 4.6254\n",
      "Epoch [632/1000], Training Loss: 4.6259\n",
      "Epoch [633/1000], Training Loss: 4.6251\n",
      "Epoch [634/1000], Training Loss: 4.6264\n",
      "Epoch [635/1000], Training Loss: 4.6242\n",
      "Epoch [636/1000], Training Loss: 4.6237\n",
      "Epoch [637/1000], Training Loss: 4.6269\n",
      "Epoch [638/1000], Training Loss: 4.6255\n",
      "Epoch [639/1000], Training Loss: 4.6205\n",
      "Epoch [640/1000], Training Loss: 4.6258\n",
      "Epoch [641/1000], Training Loss: 4.6242\n",
      "Epoch [642/1000], Training Loss: 4.6234\n",
      "Epoch [643/1000], Training Loss: 4.6219\n",
      "Epoch [644/1000], Training Loss: 4.6261\n",
      "Epoch [645/1000], Training Loss: 4.6232\n",
      "Epoch [646/1000], Training Loss: 4.6237\n",
      "Epoch [647/1000], Training Loss: 4.6221\n",
      "Epoch [648/1000], Training Loss: 4.6219\n",
      "Epoch [649/1000], Training Loss: 4.6217\n",
      "Epoch [650/1000], Training Loss: 4.6209\n",
      "Epoch [651/1000], Training Loss: 4.6241\n",
      "Epoch [652/1000], Training Loss: 4.6241\n",
      "Epoch [653/1000], Training Loss: 4.6191\n",
      "Epoch [654/1000], Training Loss: 4.6183\n",
      "Epoch [655/1000], Training Loss: 4.6208\n",
      "Epoch [656/1000], Training Loss: 4.6225\n",
      "Epoch [657/1000], Training Loss: 4.6195\n",
      "Epoch [658/1000], Training Loss: 4.6191\n",
      "Epoch [659/1000], Training Loss: 4.6197\n",
      "Epoch [660/1000], Training Loss: 4.6215\n",
      "Epoch [661/1000], Training Loss: 4.6181\n",
      "Epoch [662/1000], Training Loss: 4.6206\n",
      "Epoch [663/1000], Training Loss: 4.6178\n",
      "Epoch [664/1000], Training Loss: 4.6201\n",
      "Epoch [665/1000], Training Loss: 4.6185\n",
      "Epoch [666/1000], Training Loss: 4.6200\n",
      "Epoch [667/1000], Training Loss: 4.6216\n",
      "Epoch [668/1000], Training Loss: 4.6163\n",
      "Epoch [669/1000], Training Loss: 4.6172\n",
      "Epoch [670/1000], Training Loss: 4.6178\n",
      "Epoch [671/1000], Training Loss: 4.6192\n",
      "Epoch [672/1000], Training Loss: 4.6168\n",
      "Epoch [673/1000], Training Loss: 4.6194\n",
      "Epoch [674/1000], Training Loss: 4.6161\n",
      "Epoch [675/1000], Training Loss: 4.6160\n",
      "Epoch [676/1000], Training Loss: 4.6162\n",
      "Epoch [677/1000], Training Loss: 4.6172\n",
      "Epoch [678/1000], Training Loss: 4.6154\n",
      "Epoch [679/1000], Training Loss: 4.6146\n",
      "Epoch [680/1000], Training Loss: 4.6165\n",
      "Epoch [681/1000], Training Loss: 4.6170\n",
      "Epoch [682/1000], Training Loss: 4.6170\n",
      "Epoch [683/1000], Training Loss: 4.6160\n",
      "Epoch [684/1000], Training Loss: 4.6149\n",
      "Epoch [685/1000], Training Loss: 4.6154\n",
      "Epoch [686/1000], Training Loss: 4.6203\n",
      "Epoch [687/1000], Training Loss: 4.6141\n",
      "Epoch [688/1000], Training Loss: 4.6143\n",
      "Epoch [689/1000], Training Loss: 4.6159\n",
      "Epoch [690/1000], Training Loss: 4.6148\n",
      "Epoch [691/1000], Training Loss: 4.6150\n",
      "Epoch [692/1000], Training Loss: 4.6167\n",
      "Epoch [693/1000], Training Loss: 4.6139\n",
      "Epoch [694/1000], Training Loss: 4.6145\n",
      "Epoch [695/1000], Training Loss: 4.6149\n",
      "Epoch [696/1000], Training Loss: 4.6130\n",
      "Epoch [697/1000], Training Loss: 4.6169\n",
      "Epoch [698/1000], Training Loss: 4.6146\n",
      "Epoch [699/1000], Training Loss: 4.6134\n",
      "Epoch [700/1000], Training Loss: 4.6167\n",
      "Epoch [701/1000], Training Loss: 4.6120\n",
      "Epoch [702/1000], Training Loss: 4.6149\n",
      "Epoch [703/1000], Training Loss: 4.6143\n",
      "Epoch [704/1000], Training Loss: 4.6155\n",
      "Epoch [705/1000], Training Loss: 4.6137\n",
      "Epoch [706/1000], Training Loss: 4.6113\n",
      "Epoch [707/1000], Training Loss: 4.6095\n",
      "Epoch [708/1000], Training Loss: 4.6123\n",
      "Epoch [709/1000], Training Loss: 4.6136\n",
      "Epoch [710/1000], Training Loss: 4.6108\n",
      "Epoch [711/1000], Training Loss: 4.6166\n",
      "Epoch [712/1000], Training Loss: 4.6132\n",
      "Epoch [713/1000], Training Loss: 4.6100\n",
      "Epoch [714/1000], Training Loss: 4.6123\n",
      "Epoch [715/1000], Training Loss: 4.6106\n",
      "Epoch [716/1000], Training Loss: 4.6123\n",
      "Epoch [717/1000], Training Loss: 4.6113\n",
      "Epoch [718/1000], Training Loss: 4.6128\n",
      "Epoch [719/1000], Training Loss: 4.6105\n",
      "Epoch [720/1000], Training Loss: 4.6125\n",
      "Epoch [721/1000], Training Loss: 4.6105\n",
      "Epoch [722/1000], Training Loss: 4.6091\n",
      "Epoch [723/1000], Training Loss: 4.6107\n",
      "Epoch [724/1000], Training Loss: 4.6093\n",
      "Epoch [725/1000], Training Loss: 4.6100\n",
      "Epoch [726/1000], Training Loss: 4.6097\n",
      "Epoch [727/1000], Training Loss: 4.6103\n",
      "Epoch [728/1000], Training Loss: 4.6082\n",
      "Epoch [729/1000], Training Loss: 4.6059\n",
      "Epoch [730/1000], Training Loss: 4.6080\n",
      "Epoch [731/1000], Training Loss: 4.6092\n",
      "Epoch [732/1000], Training Loss: 4.6092\n",
      "Epoch [733/1000], Training Loss: 4.6096\n",
      "Epoch [734/1000], Training Loss: 4.6093\n",
      "Epoch [735/1000], Training Loss: 4.6072\n",
      "Epoch [736/1000], Training Loss: 4.6066\n",
      "Epoch [737/1000], Training Loss: 4.6082\n",
      "Epoch [738/1000], Training Loss: 4.6103\n",
      "Epoch [739/1000], Training Loss: 4.6100\n",
      "Epoch [740/1000], Training Loss: 4.6078\n",
      "Epoch [741/1000], Training Loss: 4.6091\n",
      "Epoch [742/1000], Training Loss: 4.6083\n",
      "Epoch [743/1000], Training Loss: 4.6054\n",
      "Epoch [744/1000], Training Loss: 4.6072\n",
      "Epoch [745/1000], Training Loss: 4.6087\n",
      "Epoch [746/1000], Training Loss: 4.6057\n",
      "Epoch [747/1000], Training Loss: 4.6077\n",
      "Epoch [748/1000], Training Loss: 4.6059\n",
      "Epoch [749/1000], Training Loss: 4.6063\n",
      "Epoch [750/1000], Training Loss: 4.6031\n",
      "Epoch [751/1000], Training Loss: 4.6065\n",
      "Epoch [752/1000], Training Loss: 4.6057\n",
      "Epoch [753/1000], Training Loss: 4.6060\n",
      "Epoch [754/1000], Training Loss: 4.6041\n",
      "Epoch [755/1000], Training Loss: 4.6056\n",
      "Epoch [756/1000], Training Loss: 4.6052\n",
      "Epoch [757/1000], Training Loss: 4.6043\n",
      "Epoch [758/1000], Training Loss: 4.6045\n",
      "Epoch [759/1000], Training Loss: 4.6043\n",
      "Epoch [760/1000], Training Loss: 4.6037\n",
      "Epoch [761/1000], Training Loss: 4.6041\n",
      "Epoch [762/1000], Training Loss: 4.6029\n",
      "Epoch [763/1000], Training Loss: 4.6059\n",
      "Epoch [764/1000], Training Loss: 4.6041\n",
      "Epoch [765/1000], Training Loss: 4.6030\n",
      "Epoch [766/1000], Training Loss: 4.6038\n",
      "Epoch [767/1000], Training Loss: 4.6033\n",
      "Epoch [768/1000], Training Loss: 4.6022\n",
      "Epoch [769/1000], Training Loss: 4.6061\n",
      "Epoch [770/1000], Training Loss: 4.6017\n",
      "Epoch [771/1000], Training Loss: 4.6030\n",
      "Epoch [772/1000], Training Loss: 4.6025\n",
      "Epoch [773/1000], Training Loss: 4.6022\n",
      "Epoch [774/1000], Training Loss: 4.6040\n",
      "Epoch [775/1000], Training Loss: 4.6034\n",
      "Epoch [776/1000], Training Loss: 4.6024\n",
      "Epoch [777/1000], Training Loss: 4.6010\n",
      "Epoch [778/1000], Training Loss: 4.5998\n",
      "Epoch [779/1000], Training Loss: 4.6003\n",
      "Epoch [780/1000], Training Loss: 4.6023\n",
      "Epoch [781/1000], Training Loss: 4.6009\n",
      "Epoch [782/1000], Training Loss: 4.6034\n",
      "Epoch [783/1000], Training Loss: 4.5988\n",
      "Epoch [784/1000], Training Loss: 4.6030\n",
      "Epoch [785/1000], Training Loss: 4.6004\n",
      "Epoch [786/1000], Training Loss: 4.5949\n",
      "Epoch [787/1000], Training Loss: 4.6010\n",
      "Epoch [788/1000], Training Loss: 4.5990\n",
      "Epoch [789/1000], Training Loss: 4.5991\n",
      "Epoch [790/1000], Training Loss: 4.6007\n",
      "Epoch [791/1000], Training Loss: 4.5991\n",
      "Epoch [792/1000], Training Loss: 4.6004\n",
      "Epoch [793/1000], Training Loss: 4.5977\n",
      "Epoch [794/1000], Training Loss: 4.5992\n",
      "Epoch [795/1000], Training Loss: 4.5993\n",
      "Epoch [796/1000], Training Loss: 4.6004\n",
      "Epoch [797/1000], Training Loss: 4.5982\n",
      "Epoch [798/1000], Training Loss: 4.5992\n",
      "Epoch [799/1000], Training Loss: 4.5958\n",
      "Epoch [800/1000], Training Loss: 4.6020\n",
      "Epoch [801/1000], Training Loss: 4.6009\n",
      "Epoch [802/1000], Training Loss: 4.5976\n",
      "Epoch [803/1000], Training Loss: 4.6000\n",
      "Epoch [804/1000], Training Loss: 4.5978\n",
      "Epoch [805/1000], Training Loss: 4.5984\n",
      "Epoch [806/1000], Training Loss: 4.5972\n",
      "Epoch [807/1000], Training Loss: 4.5967\n",
      "Epoch [808/1000], Training Loss: 4.5974\n",
      "Epoch [809/1000], Training Loss: 4.6000\n",
      "Epoch [810/1000], Training Loss: 4.5980\n",
      "Epoch [811/1000], Training Loss: 4.5972\n",
      "Epoch [812/1000], Training Loss: 4.5967\n",
      "Epoch [813/1000], Training Loss: 4.5978\n",
      "Epoch [814/1000], Training Loss: 4.5968\n",
      "Epoch [815/1000], Training Loss: 4.5978\n",
      "Epoch [816/1000], Training Loss: 4.5986\n",
      "Epoch [817/1000], Training Loss: 4.5937\n",
      "Epoch [818/1000], Training Loss: 4.5963\n",
      "Epoch [819/1000], Training Loss: 4.5964\n",
      "Epoch [820/1000], Training Loss: 4.5993\n",
      "Epoch [821/1000], Training Loss: 4.5977\n",
      "Epoch [822/1000], Training Loss: 4.5948\n",
      "Epoch [823/1000], Training Loss: 4.5965\n",
      "Epoch [824/1000], Training Loss: 4.5973\n",
      "Epoch [825/1000], Training Loss: 4.5986\n",
      "Epoch [826/1000], Training Loss: 4.5927\n",
      "Epoch [827/1000], Training Loss: 4.5932\n",
      "Epoch [828/1000], Training Loss: 4.5939\n",
      "Epoch [829/1000], Training Loss: 4.5954\n",
      "Epoch [830/1000], Training Loss: 4.5971\n",
      "Epoch [831/1000], Training Loss: 4.5956\n",
      "Epoch [832/1000], Training Loss: 4.5948\n",
      "Epoch [833/1000], Training Loss: 4.5973\n",
      "Epoch [834/1000], Training Loss: 4.5932\n",
      "Epoch [835/1000], Training Loss: 4.5959\n",
      "Epoch [836/1000], Training Loss: 4.5939\n",
      "Epoch [837/1000], Training Loss: 4.5968\n",
      "Epoch [838/1000], Training Loss: 4.5947\n",
      "Epoch [839/1000], Training Loss: 4.5982\n",
      "Epoch [840/1000], Training Loss: 4.5948\n",
      "Epoch [841/1000], Training Loss: 4.5921\n",
      "Epoch [842/1000], Training Loss: 4.5954\n",
      "Epoch [843/1000], Training Loss: 4.5969\n",
      "Epoch [844/1000], Training Loss: 4.5948\n",
      "Epoch [845/1000], Training Loss: 4.5935\n",
      "Epoch [846/1000], Training Loss: 4.5920\n",
      "Epoch [847/1000], Training Loss: 4.5929\n",
      "Epoch [848/1000], Training Loss: 4.5930\n",
      "Epoch [849/1000], Training Loss: 4.5915\n",
      "Epoch [850/1000], Training Loss: 4.5913\n",
      "Epoch [851/1000], Training Loss: 4.5968\n",
      "Epoch [852/1000], Training Loss: 4.5947\n",
      "Epoch [853/1000], Training Loss: 4.5941\n",
      "Epoch [854/1000], Training Loss: 4.5940\n",
      "Epoch [855/1000], Training Loss: 4.5927\n",
      "Epoch [856/1000], Training Loss: 4.5911\n",
      "Epoch [857/1000], Training Loss: 4.5894\n",
      "Epoch [858/1000], Training Loss: 4.5931\n",
      "Epoch [859/1000], Training Loss: 4.5925\n",
      "Epoch [860/1000], Training Loss: 4.5902\n",
      "Epoch [861/1000], Training Loss: 4.5897\n",
      "Epoch [862/1000], Training Loss: 4.5906\n",
      "Epoch [863/1000], Training Loss: 4.5933\n",
      "Epoch [864/1000], Training Loss: 4.5938\n",
      "Epoch [865/1000], Training Loss: 4.5912\n",
      "Epoch [866/1000], Training Loss: 4.5914\n",
      "Epoch [867/1000], Training Loss: 4.5898\n",
      "Epoch [868/1000], Training Loss: 4.5877\n",
      "Epoch [869/1000], Training Loss: 4.5923\n",
      "Epoch [870/1000], Training Loss: 4.5917\n",
      "Epoch [871/1000], Training Loss: 4.5922\n",
      "Epoch [872/1000], Training Loss: 4.5886\n",
      "Epoch [873/1000], Training Loss: 4.5919\n",
      "Epoch [874/1000], Training Loss: 4.5897\n",
      "Epoch [875/1000], Training Loss: 4.5915\n",
      "Epoch [876/1000], Training Loss: 4.5917\n",
      "Epoch [877/1000], Training Loss: 4.5925\n",
      "Epoch [878/1000], Training Loss: 4.5924\n",
      "Epoch [879/1000], Training Loss: 4.5885\n",
      "Epoch [880/1000], Training Loss: 4.5914\n",
      "Epoch [881/1000], Training Loss: 4.5939\n",
      "Epoch [882/1000], Training Loss: 4.5892\n",
      "Epoch [883/1000], Training Loss: 4.5912\n",
      "Epoch [884/1000], Training Loss: 4.5940\n",
      "Epoch [885/1000], Training Loss: 4.5907\n",
      "Epoch [886/1000], Training Loss: 4.5886\n",
      "Epoch [887/1000], Training Loss: 4.5903\n",
      "Epoch [888/1000], Training Loss: 4.5912\n",
      "Epoch [889/1000], Training Loss: 4.5888\n",
      "Epoch [890/1000], Training Loss: 4.5895\n",
      "Epoch [891/1000], Training Loss: 4.5903\n",
      "Epoch [892/1000], Training Loss: 4.5896\n",
      "Epoch [893/1000], Training Loss: 4.5871\n",
      "Epoch [894/1000], Training Loss: 4.5877\n",
      "Epoch [895/1000], Training Loss: 4.5896\n",
      "Epoch [896/1000], Training Loss: 4.5922\n",
      "Epoch [897/1000], Training Loss: 4.5879\n",
      "Epoch [898/1000], Training Loss: 4.5880\n",
      "Epoch [899/1000], Training Loss: 4.5878\n",
      "Epoch [900/1000], Training Loss: 4.5891\n",
      "Epoch [901/1000], Training Loss: 4.5892\n",
      "Epoch [902/1000], Training Loss: 4.5891\n",
      "Epoch [903/1000], Training Loss: 4.5900\n",
      "Epoch [904/1000], Training Loss: 4.5931\n",
      "Epoch [905/1000], Training Loss: 4.5895\n",
      "Epoch [906/1000], Training Loss: 4.5888\n",
      "Epoch [907/1000], Training Loss: 4.5895\n",
      "Epoch [908/1000], Training Loss: 4.5886\n",
      "Epoch [909/1000], Training Loss: 4.5883\n",
      "Epoch [910/1000], Training Loss: 4.5885\n",
      "Epoch [911/1000], Training Loss: 4.5882\n",
      "Epoch [912/1000], Training Loss: 4.5873\n",
      "Epoch [913/1000], Training Loss: 4.5892\n",
      "Epoch [914/1000], Training Loss: 4.5905\n",
      "Epoch [915/1000], Training Loss: 4.5865\n",
      "Epoch [916/1000], Training Loss: 4.5872\n",
      "Epoch [917/1000], Training Loss: 4.5864\n",
      "Epoch [918/1000], Training Loss: 4.5862\n",
      "Epoch [919/1000], Training Loss: 4.5869\n",
      "Epoch [920/1000], Training Loss: 4.5873\n",
      "Epoch [921/1000], Training Loss: 4.5889\n",
      "Epoch [922/1000], Training Loss: 4.5875\n",
      "Epoch [923/1000], Training Loss: 4.5861\n",
      "Epoch [924/1000], Training Loss: 4.5852\n",
      "Epoch [925/1000], Training Loss: 4.5868\n",
      "Epoch [926/1000], Training Loss: 4.5866\n",
      "Epoch [927/1000], Training Loss: 4.5863\n",
      "Epoch [928/1000], Training Loss: 4.5858\n",
      "Epoch [929/1000], Training Loss: 4.5912\n",
      "Epoch [930/1000], Training Loss: 4.5848\n",
      "Epoch [931/1000], Training Loss: 4.5863\n",
      "Epoch [932/1000], Training Loss: 4.5863\n",
      "Epoch [933/1000], Training Loss: 4.5843\n",
      "Epoch [934/1000], Training Loss: 4.5843\n",
      "Epoch [935/1000], Training Loss: 4.5853\n",
      "Epoch [936/1000], Training Loss: 4.5875\n",
      "Epoch [937/1000], Training Loss: 4.5863\n",
      "Epoch [938/1000], Training Loss: 4.5864\n",
      "Epoch [939/1000], Training Loss: 4.5862\n",
      "Epoch [940/1000], Training Loss: 4.5855\n",
      "Epoch [941/1000], Training Loss: 4.5849\n",
      "Epoch [942/1000], Training Loss: 4.5838\n",
      "Epoch [943/1000], Training Loss: 4.5847\n",
      "Epoch [944/1000], Training Loss: 4.5888\n",
      "Epoch [945/1000], Training Loss: 4.5867\n",
      "Epoch [946/1000], Training Loss: 4.5851\n",
      "Epoch [947/1000], Training Loss: 4.5849\n",
      "Epoch [948/1000], Training Loss: 4.5867\n",
      "Epoch [949/1000], Training Loss: 4.5874\n",
      "Epoch [950/1000], Training Loss: 4.5881\n",
      "Epoch [951/1000], Training Loss: 4.5901\n",
      "Epoch [952/1000], Training Loss: 4.5881\n",
      "Epoch [953/1000], Training Loss: 4.5855\n",
      "Epoch [954/1000], Training Loss: 4.5884\n",
      "Epoch [955/1000], Training Loss: 4.5857\n",
      "Epoch [956/1000], Training Loss: 4.5870\n",
      "Epoch [957/1000], Training Loss: 4.5857\n",
      "Epoch [958/1000], Training Loss: 4.5880\n",
      "Epoch [959/1000], Training Loss: 4.5854\n",
      "Epoch [960/1000], Training Loss: 4.5869\n",
      "Epoch [961/1000], Training Loss: 4.5831\n",
      "Epoch [962/1000], Training Loss: 4.5861\n",
      "Epoch [963/1000], Training Loss: 4.5841\n",
      "Epoch [964/1000], Training Loss: 4.5830\n",
      "Epoch [965/1000], Training Loss: 4.5857\n",
      "Epoch [966/1000], Training Loss: 4.5852\n",
      "Epoch [967/1000], Training Loss: 4.5853\n",
      "Epoch [968/1000], Training Loss: 4.5852\n",
      "Epoch [969/1000], Training Loss: 4.5855\n",
      "Epoch [970/1000], Training Loss: 4.5871\n",
      "Epoch [971/1000], Training Loss: 4.5877\n",
      "Epoch [972/1000], Training Loss: 4.5837\n",
      "Epoch [973/1000], Training Loss: 4.5878\n",
      "Epoch [974/1000], Training Loss: 4.5869\n",
      "Epoch [975/1000], Training Loss: 4.5879\n",
      "Epoch [976/1000], Training Loss: 4.5862\n",
      "Epoch [977/1000], Training Loss: 4.5850\n",
      "Epoch [978/1000], Training Loss: 4.5823\n",
      "Epoch [979/1000], Training Loss: 4.5872\n",
      "Epoch [980/1000], Training Loss: 4.5879\n",
      "Epoch [981/1000], Training Loss: 4.5864\n",
      "Epoch [982/1000], Training Loss: 4.5878\n",
      "Epoch [983/1000], Training Loss: 4.5855\n",
      "Epoch [984/1000], Training Loss: 4.5873\n",
      "Epoch [985/1000], Training Loss: 4.5872\n",
      "Epoch [986/1000], Training Loss: 4.5864\n",
      "Epoch [987/1000], Training Loss: 4.5855\n",
      "Epoch [988/1000], Training Loss: 4.5896\n",
      "Epoch [989/1000], Training Loss: 4.5864\n",
      "Epoch [990/1000], Training Loss: 4.5858\n",
      "Epoch [991/1000], Training Loss: 4.5845\n",
      "Epoch [992/1000], Training Loss: 4.5858\n",
      "Epoch [993/1000], Training Loss: 4.5870\n",
      "Epoch [994/1000], Training Loss: 4.5849\n",
      "Epoch [995/1000], Training Loss: 4.5865\n",
      "Epoch [996/1000], Training Loss: 4.5830\n",
      "Epoch [997/1000], Training Loss: 4.5856\n",
      "Epoch [998/1000], Training Loss: 4.5855\n",
      "Epoch [999/1000], Training Loss: 4.5865\n",
      "Epoch [1000/1000], Training Loss: 4.5868\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.6\n",
    "momentum = 0.9\n",
    "temperature = 0.5  \n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize components\n",
    "model = SimCLRModel(resnet18, projection_dim=128).to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_decay=1.0e-6,\n",
    "        nesterov=True)\n",
    "\n",
    "# cosine annealing lr\n",
    "scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: get_lr(  \n",
    "        step,\n",
    "        num_epochs * len(train_loader),\n",
    "        learning_rate,\n",
    "        1e-3))\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 5\n",
    "min_val_loss = float('inf')\n",
    "epsilon = 1e-3\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    for (x, _) in train_loader:\n",
    "        # Reshape to combine the images into a single batch\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0] * 2, sizes[2], sizes[3], sizes[4]).cuda(non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        _, projections = model(x)\n",
    "\n",
    "        # Calculate loss using the projections\n",
    "        loss = nt_xent(projections, temperature)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "          \n",
    "    # Save model every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        save_path = f'simclr_resnet18_epoch{epoch + 1}.pt'\n",
    "        torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simclr_model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained SimCLR model\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder: torch.nn.Module, input_size, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.encoder(x))\n",
    "\n",
    "# Initialize the linear classifier\n",
    "linear_classifier = LinearClassifier(model.encoder, input_size=512, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations and Dataset\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(32), \n",
    "                                      transforms.RandomHorizontalFlip(p=0.5), \n",
    "                                      transforms.ToTensor()])\n",
    "test_transform = transforms.ToTensor()\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# DataLoaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.1336, Validation Loss: 1.7813\n",
      "Epoch [2/1000], Training Loss: 1.5562, Validation Loss: 1.2413\n",
      "Epoch [3/1000], Training Loss: 1.1172, Validation Loss: 0.9210\n",
      "Epoch [4/1000], Training Loss: 0.8675, Validation Loss: 0.7551\n",
      "Epoch [5/1000], Training Loss: 0.7492, Validation Loss: 0.6661\n",
      "Epoch [6/1000], Training Loss: 0.6804, Validation Loss: 0.6081\n",
      "Epoch [7/1000], Training Loss: 0.6230, Validation Loss: 0.5717\n",
      "Epoch [8/1000], Training Loss: 0.5840, Validation Loss: 0.5488\n",
      "Epoch [9/1000], Training Loss: 0.5693, Validation Loss: 0.5308\n",
      "Epoch [10/1000], Training Loss: 0.5308, Validation Loss: 0.5138\n",
      "Epoch [11/1000], Training Loss: 0.5564, Validation Loss: 0.5015\n",
      "Epoch [12/1000], Training Loss: 0.5297, Validation Loss: 0.4925\n",
      "Epoch [13/1000], Training Loss: 0.5519, Validation Loss: 0.4809\n",
      "Epoch [14/1000], Training Loss: 0.5311, Validation Loss: 0.4735\n",
      "Epoch [15/1000], Training Loss: 0.4960, Validation Loss: 0.4679\n",
      "Epoch [16/1000], Training Loss: 0.4965, Validation Loss: 0.4638\n",
      "Epoch [17/1000], Training Loss: 0.4726, Validation Loss: 0.4572\n",
      "Epoch [18/1000], Training Loss: 0.5025, Validation Loss: 0.4485\n",
      "Epoch [19/1000], Training Loss: 0.4879, Validation Loss: 0.4453\n",
      "Epoch [20/1000], Training Loss: 0.4735, Validation Loss: 0.4409\n",
      "Epoch [21/1000], Training Loss: 0.4891, Validation Loss: 0.4376\n",
      "Epoch [22/1000], Training Loss: 0.4641, Validation Loss: 0.4346\n",
      "Epoch [23/1000], Training Loss: 0.4967, Validation Loss: 0.4318\n",
      "Epoch [24/1000], Training Loss: 0.4559, Validation Loss: 0.4285\n",
      "Epoch [25/1000], Training Loss: 0.4678, Validation Loss: 0.4264\n",
      "Epoch [26/1000], Training Loss: 0.4447, Validation Loss: 0.4248\n",
      "Epoch [27/1000], Training Loss: 0.4578, Validation Loss: 0.4224\n",
      "Epoch [28/1000], Training Loss: 0.4612, Validation Loss: 0.4196\n",
      "Epoch [29/1000], Training Loss: 0.4419, Validation Loss: 0.4172\n",
      "Epoch [30/1000], Training Loss: 0.4555, Validation Loss: 0.4147\n",
      "Epoch [31/1000], Training Loss: 0.4408, Validation Loss: 0.4126\n",
      "Epoch [32/1000], Training Loss: 0.4154, Validation Loss: 0.4130\n",
      "Epoch [33/1000], Training Loss: 0.4559, Validation Loss: 0.4105\n",
      "Epoch [34/1000], Training Loss: 0.4419, Validation Loss: 0.4078\n",
      "Epoch [35/1000], Training Loss: 0.4412, Validation Loss: 0.4048\n",
      "Epoch [36/1000], Training Loss: 0.4506, Validation Loss: 0.4039\n",
      "Epoch [37/1000], Training Loss: 0.4328, Validation Loss: 0.4036\n",
      "Epoch [38/1000], Training Loss: 0.4556, Validation Loss: 0.4005\n",
      "Epoch [39/1000], Training Loss: 0.4313, Validation Loss: 0.3991\n",
      "Epoch [40/1000], Training Loss: 0.4374, Validation Loss: 0.3967\n",
      "Epoch [41/1000], Training Loss: 0.4439, Validation Loss: 0.3965\n",
      "Epoch [42/1000], Training Loss: 0.4176, Validation Loss: 0.3947\n",
      "Epoch [43/1000], Training Loss: 0.4347, Validation Loss: 0.3916\n",
      "Epoch [44/1000], Training Loss: 0.4222, Validation Loss: 0.3918\n",
      "Epoch [45/1000], Training Loss: 0.4548, Validation Loss: 0.3905\n",
      "Epoch [46/1000], Training Loss: 0.4509, Validation Loss: 0.3891\n",
      "Epoch [47/1000], Training Loss: 0.4128, Validation Loss: 0.3882\n",
      "Epoch [48/1000], Training Loss: 0.4190, Validation Loss: 0.3867\n",
      "Epoch [49/1000], Training Loss: 0.4387, Validation Loss: 0.3869\n",
      "Epoch [50/1000], Training Loss: 0.4036, Validation Loss: 0.3861\n",
      "Epoch [51/1000], Training Loss: 0.4068, Validation Loss: 0.3842\n",
      "Epoch [52/1000], Training Loss: 0.4280, Validation Loss: 0.3833\n",
      "Epoch [53/1000], Training Loss: 0.4267, Validation Loss: 0.3841\n",
      "Epoch [54/1000], Training Loss: 0.4139, Validation Loss: 0.3823\n",
      "Epoch [55/1000], Training Loss: 0.4152, Validation Loss: 0.3817\n",
      "Epoch [56/1000], Training Loss: 0.4311, Validation Loss: 0.3802\n",
      "Epoch [57/1000], Training Loss: 0.4155, Validation Loss: 0.3808\n",
      "Epoch [58/1000], Training Loss: 0.4070, Validation Loss: 0.3804\n",
      "Epoch [59/1000], Training Loss: 0.3954, Validation Loss: 0.3780\n",
      "Epoch [60/1000], Training Loss: 0.4277, Validation Loss: 0.3765\n",
      "Epoch [61/1000], Training Loss: 0.4135, Validation Loss: 0.3753\n",
      "Epoch [62/1000], Training Loss: 0.3922, Validation Loss: 0.3749\n",
      "Epoch [63/1000], Training Loss: 0.3865, Validation Loss: 0.3750\n",
      "Epoch [64/1000], Training Loss: 0.4079, Validation Loss: 0.3742\n",
      "Epoch [65/1000], Training Loss: 0.4019, Validation Loss: 0.3728\n",
      "Epoch [66/1000], Training Loss: 0.3889, Validation Loss: 0.3727\n",
      "Epoch [67/1000], Training Loss: 0.4207, Validation Loss: 0.3730\n",
      "Epoch [68/1000], Training Loss: 0.4084, Validation Loss: 0.3717\n",
      "Epoch [69/1000], Training Loss: 0.3913, Validation Loss: 0.3724\n",
      "Epoch [70/1000], Training Loss: 0.4207, Validation Loss: 0.3745\n",
      "Epoch [71/1000], Training Loss: 0.4039, Validation Loss: 0.3722\n",
      "Epoch [72/1000], Training Loss: 0.4174, Validation Loss: 0.3691\n",
      "Epoch [73/1000], Training Loss: 0.4330, Validation Loss: 0.3679\n",
      "Epoch [74/1000], Training Loss: 0.3758, Validation Loss: 0.3679\n",
      "Epoch [75/1000], Training Loss: 0.4111, Validation Loss: 0.3686\n",
      "Epoch [76/1000], Training Loss: 0.4040, Validation Loss: 0.3672\n",
      "Epoch [77/1000], Training Loss: 0.4075, Validation Loss: 0.3661\n",
      "Epoch [78/1000], Training Loss: 0.4051, Validation Loss: 0.3657\n",
      "Epoch [79/1000], Training Loss: 0.4054, Validation Loss: 0.3647\n",
      "Epoch [80/1000], Training Loss: 0.3795, Validation Loss: 0.3646\n",
      "Epoch [81/1000], Training Loss: 0.3934, Validation Loss: 0.3648\n",
      "Epoch [82/1000], Training Loss: 0.4197, Validation Loss: 0.3642\n",
      "Epoch [83/1000], Training Loss: 0.4078, Validation Loss: 0.3635\n",
      "Epoch [84/1000], Training Loss: 0.3908, Validation Loss: 0.3647\n",
      "Epoch [85/1000], Training Loss: 0.4330, Validation Loss: 0.3635\n",
      "Epoch [86/1000], Training Loss: 0.4030, Validation Loss: 0.3641\n",
      "Epoch [87/1000], Training Loss: 0.4021, Validation Loss: 0.3631\n",
      "Epoch [88/1000], Training Loss: 0.4021, Validation Loss: 0.3633\n",
      "Epoch [89/1000], Training Loss: 0.3880, Validation Loss: 0.3630\n",
      "Epoch [90/1000], Training Loss: 0.3960, Validation Loss: 0.3624\n",
      "Epoch [91/1000], Training Loss: 0.4060, Validation Loss: 0.3618\n",
      "Epoch [92/1000], Training Loss: 0.4060, Validation Loss: 0.3600\n",
      "Epoch [93/1000], Training Loss: 0.3763, Validation Loss: 0.3599\n",
      "Epoch [94/1000], Training Loss: 0.3818, Validation Loss: 0.3597\n",
      "Epoch [95/1000], Training Loss: 0.3915, Validation Loss: 0.3577\n",
      "Epoch [96/1000], Training Loss: 0.3963, Validation Loss: 0.3593\n",
      "Epoch [97/1000], Training Loss: 0.4008, Validation Loss: 0.3592\n",
      "Epoch [98/1000], Training Loss: 0.4089, Validation Loss: 0.3614\n",
      "Epoch [99/1000], Training Loss: 0.3689, Validation Loss: 0.3622\n",
      "Epoch [100/1000], Training Loss: 0.3860, Validation Loss: 0.3609\n",
      "Epoch [101/1000], Training Loss: 0.3651, Validation Loss: 0.3596\n",
      "Epoch [102/1000], Training Loss: 0.3914, Validation Loss: 0.3585\n",
      "Epoch [103/1000], Training Loss: 0.3819, Validation Loss: 0.3572\n",
      "Epoch [104/1000], Training Loss: 0.3834, Validation Loss: 0.3572\n",
      "Epoch [105/1000], Training Loss: 0.3654, Validation Loss: 0.3573\n",
      "Epoch [106/1000], Training Loss: 0.4003, Validation Loss: 0.3565\n",
      "Epoch [107/1000], Training Loss: 0.3778, Validation Loss: 0.3569\n",
      "Epoch [108/1000], Training Loss: 0.3804, Validation Loss: 0.3569\n",
      "Epoch [109/1000], Training Loss: 0.3602, Validation Loss: 0.3570\n",
      "Epoch [110/1000], Training Loss: 0.3924, Validation Loss: 0.3562\n",
      "Epoch [111/1000], Training Loss: 0.3729, Validation Loss: 0.3549\n",
      "Epoch [112/1000], Training Loss: 0.3840, Validation Loss: 0.3557\n",
      "Epoch [113/1000], Training Loss: 0.4194, Validation Loss: 0.3556\n",
      "Epoch [114/1000], Training Loss: 0.3985, Validation Loss: 0.3541\n",
      "Epoch [115/1000], Training Loss: 0.4234, Validation Loss: 0.3524\n",
      "Epoch [116/1000], Training Loss: 0.4090, Validation Loss: 0.3509\n",
      "Epoch [117/1000], Training Loss: 0.4049, Validation Loss: 0.3507\n",
      "Epoch [118/1000], Training Loss: 0.3783, Validation Loss: 0.3523\n",
      "Epoch [119/1000], Training Loss: 0.3808, Validation Loss: 0.3512\n",
      "Epoch [120/1000], Training Loss: 0.3896, Validation Loss: 0.3510\n",
      "Epoch [121/1000], Training Loss: 0.3952, Validation Loss: 0.3507\n",
      "Epoch [122/1000], Training Loss: 0.3793, Validation Loss: 0.3506\n",
      "Epoch [123/1000], Training Loss: 0.3720, Validation Loss: 0.3502\n",
      "Epoch [124/1000], Training Loss: 0.3731, Validation Loss: 0.3497\n",
      "Epoch [125/1000], Training Loss: 0.4071, Validation Loss: 0.3497\n",
      "Epoch [126/1000], Training Loss: 0.4033, Validation Loss: 0.3511\n",
      "Epoch [127/1000], Training Loss: 0.3778, Validation Loss: 0.3508\n",
      "Epoch [128/1000], Training Loss: 0.3806, Validation Loss: 0.3508\n",
      "Epoch [129/1000], Training Loss: 0.3957, Validation Loss: 0.3496\n",
      "Epoch [130/1000], Training Loss: 0.3769, Validation Loss: 0.3489\n",
      "Epoch [131/1000], Training Loss: 0.3865, Validation Loss: 0.3498\n",
      "Epoch [132/1000], Training Loss: 0.3872, Validation Loss: 0.3504\n",
      "Epoch [133/1000], Training Loss: 0.3992, Validation Loss: 0.3503\n",
      "Epoch [134/1000], Training Loss: 0.3560, Validation Loss: 0.3503\n",
      "Epoch [135/1000], Training Loss: 0.3676, Validation Loss: 0.3492\n",
      "Epoch [136/1000], Training Loss: 0.3981, Validation Loss: 0.3486\n",
      "Epoch [137/1000], Training Loss: 0.3938, Validation Loss: 0.3503\n",
      "Epoch [138/1000], Training Loss: 0.3624, Validation Loss: 0.3504\n",
      "Epoch [139/1000], Training Loss: 0.4008, Validation Loss: 0.3506\n",
      "Epoch [140/1000], Training Loss: 0.3812, Validation Loss: 0.3500\n",
      "Epoch [141/1000], Training Loss: 0.3816, Validation Loss: 0.3497\n",
      "Epoch [142/1000], Training Loss: 0.3653, Validation Loss: 0.3485\n",
      "Epoch [143/1000], Training Loss: 0.3739, Validation Loss: 0.3472\n",
      "Epoch [144/1000], Training Loss: 0.3752, Validation Loss: 0.3465\n",
      "Epoch [145/1000], Training Loss: 0.3739, Validation Loss: 0.3452\n",
      "Epoch [146/1000], Training Loss: 0.3561, Validation Loss: 0.3454\n",
      "Epoch [147/1000], Training Loss: 0.4029, Validation Loss: 0.3459\n",
      "Epoch [148/1000], Training Loss: 0.3993, Validation Loss: 0.3457\n",
      "Epoch [149/1000], Training Loss: 0.3786, Validation Loss: 0.3464\n",
      "Epoch [150/1000], Training Loss: 0.3398, Validation Loss: 0.3460\n",
      "Epoch [151/1000], Training Loss: 0.3880, Validation Loss: 0.3452\n",
      "Epoch [152/1000], Training Loss: 0.3725, Validation Loss: 0.3452\n",
      "Epoch [153/1000], Training Loss: 0.4044, Validation Loss: 0.3448\n",
      "Epoch [154/1000], Training Loss: 0.3831, Validation Loss: 0.3454\n",
      "Epoch [155/1000], Training Loss: 0.3594, Validation Loss: 0.3453\n",
      "Epoch [156/1000], Training Loss: 0.3692, Validation Loss: 0.3462\n",
      "Epoch [157/1000], Training Loss: 0.3974, Validation Loss: 0.3465\n",
      "Epoch [158/1000], Training Loss: 0.3614, Validation Loss: 0.3465\n",
      "Epoch [159/1000], Training Loss: 0.3864, Validation Loss: 0.3462\n",
      "Epoch [160/1000], Training Loss: 0.3837, Validation Loss: 0.3452\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4tUlEQVR4nO3deVhUZf8G8PvMAMMOLqyKiooLLmgupOZWFKIvuZXLz3JJs0wtU7N8zbXMtCzNel3KxBbTNJfM1NBwN9fc0kwNxQXEjV22mef3xzDHGRhg2Oaw3J/rmgvmnDNnngMIt99nOZIQQoCIiIioClEp3QAiIiIia2MAIiIioiqHAYiIiIiqHAYgIiIiqnIYgIiIiKjKYQAiIiKiKocBiIiIiKocBiAiIiKqchiAiIiIqMphAKIKafjw4ahXr16xXjtr1ixIklS6DSpnrl69CkmSEBERYfX3liQJs2bNkp9HRERAkiRcvXq10NfWq1cPw4cPL9X2lORnhai4JEnCuHHjlG4GFYABiEqVJEkWPfbs2aN0U6u8119/HZIk4fLly/keM23aNEiShDNnzlixZUV369YtzJo1C6dOnVK6KTJDCP3444+VbopFYmJi8Oqrr6JevXrQaDTw9PREnz59cPDgQaWbZlZBv19effVVpZtHFYCN0g2gyuXbb781ef7NN98gMjIyz/amTZuW6H2+/PJL6HS6Yr323XffxTvvvFOi968MhgwZgiVLlmDNmjWYMWOG2WN++OEHtGjRAi1btiz2+7z44osYNGgQNBpNsc9RmFu3bmH27NmoV68eWrVqZbKvJD8rVcXBgwfRs2dPAMCoUaMQGBiIuLg4REREoHPnzli8eDHGjx+vcCvzevrppzF06NA82xs1aqRAa6iiYQCiUvXCCy+YPP/jjz8QGRmZZ3tuaWlpcHR0tPh9bG1ti9U+ALCxsYGNDX/0g4OD0bBhQ/zwww9mA9Dhw4cRHR2NDz/8sETvo1aroVarS3SOkijJz0pV8ODBAzz33HNwcHDAwYMH0aBBA3nfxIkTERoaigkTJqBNmzbo2LGj1dqVnp4OOzs7qFT5d1Q0atSo0N8tRPlhFxhZXbdu3dC8eXOcOHECXbp0gaOjI/773/8CALZs2YJevXrB19cXGo0GDRo0wHvvvQetVmtyjtzjOoy7G1asWIEGDRpAo9GgXbt2OHbsmMlrzY0BMvTXb968Gc2bN4dGo0GzZs2wY8eOPO3fs2cP2rZtC3t7ezRo0ADLly+3eFzR/v378fzzz6NOnTrQaDTw8/PDm2++iYcPH+a5PmdnZ9y8eRN9+vSBs7MzPDw8MHny5Dxfi4SEBAwfPhxubm5wd3fHsGHDkJCQUGhbAH0V6O+//8bJkyfz7FuzZg0kScLgwYORmZmJGTNmoE2bNnBzc4OTkxM6d+6MqKioQt/D3BggIQTef/991K5dG46OjujevTv++uuvPK+9f/8+Jk+ejBYtWsDZ2Rmurq4ICwvD6dOn5WP27NmDdu3aAQBGjBghd4MYxj+ZGwOUmpqKSZMmwc/PDxqNBo0bN8bHH38MIYTJcUX5uSiu+Ph4jBw5El5eXrC3t0dQUBBWr16d57i1a9eiTZs2cHFxgaurK1q0aIHFixfL+7OysjB79mwEBATA3t4eNWrUwBNPPIHIyMgC33/58uWIi4vDRx99ZBJ+AMDBwQGrV6+GJEmYM2cOAOD48eOQJMlsG3fu3AlJkvDLL7/I227evImXXnoJXl5e8tfv66+/Nnndnj17IEkS1q5di3fffRe1atWCo6MjkpKSCv8CFsL4903Hjh3h4OAAf39/LFu2LM+xln4vdDodFi9ejBYtWsDe3h4eHh7o0aMHjh8/nufYwn52kpOTMWHCBJOux6efftrsv0kqXfxvMCni3r17CAsLw6BBg/DCCy/Ay8sLgP6PpbOzMyZOnAhnZ2f8/vvvmDFjBpKSkvDRRx8Vet41a9YgOTkZr7zyCiRJwoIFC9CvXz/8+++/hVYCDhw4gI0bN+K1116Di4sLPvvsM/Tv3x8xMTGoUaMGAODPP/9Ejx494OPjg9mzZ0Or1WLOnDnw8PCw6LrXr1+PtLQ0jBkzBjVq1MDRo0exZMkS3LhxA+vXrzc5VqvVIjQ0FMHBwfj444+xa9cuLFy4EA0aNMCYMWMA6INE7969ceDAAbz66qto2rQpNm3ahGHDhlnUniFDhmD27NlYs2YNHnvsMZP3/vHHH9G5c2fUqVMHd+/exVdffYXBgwfj5ZdfRnJyMlauXInQ0FAcPXo0T7dTYWbMmIH3338fPXv2RM+ePXHy5Ek888wzyMzMNDnu33//xebNm/H888/D398ft2/fxvLly9G1a1ecP38evr6+aNq0KebMmYMZM2Zg9OjR6Ny5MwDkW60QQuDZZ59FVFQURo4ciVatWmHnzp146623cPPmTXz66acmx1vyc1FcDx8+RLdu3XD58mWMGzcO/v7+WL9+PYYPH46EhAS88cYbAIDIyEgMHjwYTz31FObPnw8AuHDhAg4ePCgfM2vWLMybNw+jRo1C+/btkZSUhOPHj+PkyZN4+umn823D1q1bYW9vjwEDBpjd7+/vjyeeeAK///47Hj58iLZt26J+/fr48ccf8/ycrVu3DtWqVUNoaCgA4Pbt23j88cflIOnh4YHt27dj5MiRSEpKwoQJE0xe/95778HOzg6TJ09GRkYG7OzsCvz6paen4+7du3m2u7q6mrz2wYMH6NmzJwYMGIDBgwfjxx9/xJgxY2BnZ4eXXnoJgOXfCwAYOXIkIiIiEBYWhlGjRiE7Oxv79+/HH3/8gbZt28rHWfKz8+qrr2LDhg0YN24cAgMDce/ePRw4cAAXLlww+TdJZUAQlaGxY8eK3D9mXbt2FQDEsmXL8hyflpaWZ9srr7wiHB0dRXp6urxt2LBhom7duvLz6OhoAUDUqFFD3L9/X96+ZcsWAUBs3bpV3jZz5sw8bQIg7OzsxOXLl+Vtp0+fFgDEkiVL5G3h4eHC0dFR3Lx5U9526dIlYWNjk+ec5pi7vnnz5glJksS1a9dMrg+AmDNnjsmxrVu3Fm3atJGfb968WQAQCxYskLdlZ2eLzp07CwBi1apVhbapXbt2onbt2kKr1crbduzYIQCI5cuXy+fMyMgwed2DBw+El5eXeOmll0y2AxAzZ86Un69atUoAENHR0UIIIeLj44WdnZ3o1auX0Ol08nH//e9/BQAxbNgweVt6erpJu4TQf681Go3J1+bYsWP5Xm/unxXD1+z99983Oe65554TkiSZ/AxY+nNhjuFn8qOPPsr3mEWLFgkA4rvvvpO3ZWZmig4dOghnZ2eRlJQkhBDijTfeEK6uriI7OzvfcwUFBYlevXoV2CZz3N3dRVBQUIHHvP766wKAOHPmjBBCiKlTpwpbW1uTf2sZGRnC3d3d5Odh5MiRwsfHR9y9e9fkfIMGDRJubm7yv4eoqCgBQNSvX9/svxFzAOT7+OGHH+TjDL9vFi5caNLWVq1aCU9PT5GZmSmEsPx78fvvvwsA4vXXX8/TJuOfZ0t/dtzc3MTYsWMtumYqXewCI0VoNBqMGDEiz3YHBwf58+TkZNy9exedO3dGWloa/v7770LPO3DgQFSrVk1+bqgG/Pvvv4W+NiQkxKQLoGXLlnB1dZVfq9VqsWvXLvTp0we+vr7ycQ0bNkRYWFih5wdMry81NRV3795Fx44dIYTAn3/+mef43LNZOnfubHItv/76K2xsbOSKEKAfc1OUAasvvPACbty4gX379snb1qxZAzs7Ozz//PPyOQ3/o9bpdLh//z6ys7PRtm3bIpfqd+3ahczMTIwfP96k2zB3NQDQ/5wYxoBotVrcu3cPzs7OaNy4cbG7CH799Veo1Wq8/vrrJtsnTZoEIQS2b99usr2wn4uS+PXXX+Ht7Y3BgwfL22xtbfH6668jJSUFe/fuBQC4u7sjNTW1wO4sd3d3/PXXX7h06VKR2pCcnAwXF5cCjzHsN3RJDRw4EFlZWdi4caN8zG+//YaEhAQMHDgQgL7S9tNPPyE8PBxCCNy9e1d+hIaGIjExMc/3cNiwYSb/RgrTu3dvREZG5nl0797d5DgbGxu88sor8nM7Ozu88soriI+Px4kTJwBY/r346aefIEkSZs6cmac9ubvBLfnZcXd3x5EjR3Dr1i2Lr5tKBwMQKaJWrVpmy9t//fUX+vbtCzc3N7i6usLDw0Me5JiYmFjoeevUqWPy3BCGHjx4UOTXGl5veG18fDwePnyIhg0b5jnO3DZzYmJiMHz4cFSvXl0e19O1a1cAea/PMLYgv/YAwLVr1+Dj4wNnZ2eT4xo3bmxRewBg0KBBUKvVWLNmDQB9t8KmTZsQFhZmEiZXr16Nli1byuNLPDw8sG3bNou+L8auXbsGAAgICDDZ7uHhYfJ+gD5sffrppwgICIBGo0HNmjXh4eGBM2fOFPl9jd/f19c3zx99w8xEQ/sMCvu5KIlr164hICAgz0Df3G157bXX0KhRI4SFhaF27dp46aWX8owlmTNnDhISEtCoUSO0aNECb731lkXLF7i4uCA5ObnAYwz7DV+zoKAgNGnSBOvWrZOPWbduHWrWrIknn3wSAHDnzh0kJCRgxYoV8PDwMHkY/vMTHx9v8j7+/v6FttdY7dq1ERISkudh6FI38PX1hZOTk8k2w0wxw9g0S78XV65cga+vL6pXr15o+yz52VmwYAHOnTsHPz8/tG/fHrNmzSqVcE2FYwAiRZj7X15CQgK6du2K06dPY86cOdi6dSsiIyPlMQ+WTGXOb7aRyDW4tbRfawmtVounn34a27Ztw9tvv43NmzcjMjJSHqyb+/qsNXPKMOjyp59+QlZWFrZu3Yrk5GQMGTJEPua7777D8OHD0aBBA6xcuRI7duxAZGQknnzyyTKdYv7BBx9g4sSJ6NKlC7777jvs3LkTkZGRaNasmdWmtpf1z4UlPD09cerUKfz888/y+KWwsDCTMThdunTBlStX8PXXX6N58+b46quv8Nhjj+Grr74q8NxNmzbFxYsXkZGRke8xZ86cga2trUloHThwIKKionD37l1kZGTg559/Rv/+/eUZlobvzwsvvGC2ShMZGYlOnTqZvE9Rqj8VgSU/OwMGDMC///6LJUuWwNfXFx999BGaNWuWpxJJpY+DoKnc2LNnD+7du4eNGzeiS5cu8vbo6GgFW/WIp6cn7O3tzS4cWNBiggZnz57FP//8g9WrV5usXVLYLJ2C1K1bF7t370ZKSopJFejixYtFOs+QIUOwY8cObN++HWvWrIGrqyvCw8Pl/Rs2bED9+vWxceNGkzK/uW4AS9oMAJcuXUL9+vXl7Xfu3MlTVdmwYQO6d++OlStXmmxPSEhAzZo15edFWdm7bt262LVrV56uH0MXq6F91lC3bl2cOXMGOp3OpPJgri12dnYIDw9HeHg4dDodXnvtNSxfvhzTp0+XK5DVq1fHiBEjMGLECKSkpKBLly6YNWsWRo0alW8b/vOf/+Dw4cNYv3692SnlV69exf79+xESEmISUAYOHIjZs2fjp59+gpeXF5KSkjBo0CB5v4eHB1xcXKDVahESElL8L1IpuHXrFlJTU02qQP/88w8AyDMELf1eNGjQADt37sT9+/ctqgJZwsfHB6+99hpee+01xMfH47HHHsPcuXMt7lqn4mEFiMoNw/+WjP93lJmZif/9739KNcmEWq1GSEgINm/ebNJff/nyZYv+t2bu+oQQJlOZi6pnz57Izs7G0qVL5W1arRZLliwp0nn69OkDR0dH/O9//8P27dvRr18/2NvbF9j2I0eO4PDhw0Vuc0hICGxtbbFkyRKT8y1atCjPsWq1Ok+lZf369bh586bJNsMfNkum//fs2RNarRaff/65yfZPP/0UkiRZ9Y9Oz549ERcXZ9KVlJ2djSVLlsDZ2VnuHr13757J61Qqlbw4paFyk/sYZ2dnNGzYsMDKDgC88sor8PT0xFtvvZWn6yU9PR0jRoyAECLPWlFNmzZFixYtsG7dOqxbtw4+Pj4m/3FRq9Xo378/fvrpJ5w7dy7P+965c6fAdpWm7OxsLF++XH6emZmJ5cuXw8PDA23atAFg+feif//+EEJg9uzZed6nqFVBrVabpyvX09MTvr6+hX7fqORYAaJyo2PHjqhWrRqGDRsm36bh22+/tWpXQ2FmzZqF3377DZ06dcKYMWPkP6TNmzcv9DYMTZo0QYMGDTB58mTcvHkTrq6u+Omnn0o0liQ8PBydOnXCO++8g6tXryIwMBAbN24s8vgYZ2dn9OnTRx4HZNz9BeirBBs3bkTfvn3Rq1cvREdHY9myZQgMDERKSkqR3suwntG8efPwn//8Bz179sSff/6J7du3m1R1DO87Z84cjBgxAh07dsTZs2fx/fffm1SOAP3/yt3d3bFs2TK4uLjAyckJwcHBZseUhIeHo3v37pg2bRquXr2KoKAg/Pbbb9iyZQsmTJiQZy2cktq9ezfS09PzbO/Tpw9Gjx6N5cuXY/jw4Thx4gTq1auHDRs24ODBg1i0aJFcoRo1ahTu37+PJ598ErVr18a1a9ewZMkStGrVSh6jEhgYiG7duqFNmzaoXr06jh8/Lk+vLkiNGjWwYcMG9OrVC4899lielaAvX76MxYsXm11WYODAgZgxYwbs7e0xcuTIPONnPvzwQ0RFRSE4OBgvv/wyAgMDcf/+fZw8eRK7du3C/fv3i/tlBaCv4nz33Xd5tnt5eZlM/ff19cX8+fNx9epVNGrUCOvWrcOpU6ewYsUKeXkMS78X3bt3x4svvojPPvsMly5dQo8ePaDT6bB//3507969SPf/Sk5ORu3atfHcc88hKCgIzs7O2LVrF44dO4aFCxeW6GtDFrD2tDOqWvKbBt+sWTOzxx88eFA8/vjjwsHBQfj6+oopU6aInTt3CgAiKipKPi6/afDmphwj17Ts/KbBm5uKWrduXZNp2UIIsXv3btG6dWthZ2cnGjRoIL766isxadIkYW9vn89X4ZHz58+LkJAQ4ezsLGrWrClefvlleWqs8RTuYcOGCScnpzyvN9f2e/fuiRdffFG4uroKNzc38eKLL4o///zT4mnwBtu2bRMAhI+PT56p5zqdTnzwwQeibt26QqPRiNatW4tffvklz/dBiMKnwQshhFarFbNnzxY+Pj7CwcFBdOvWTZw7dy7P1zs9PV1MmjRJPq5Tp07i8OHDomvXrqJr164m77tlyxYRGBgoL0lguHZzbUxOThZvvvmm8PX1Fba2tiIgIEB89NFHJtOYDddi6c9Fboafyfwe3377rRBCiNu3b4sRI0aImjVrCjs7O9GiRYs837cNGzaIZ555Rnh6ego7OztRp04d8corr4jY2Fj5mPfff1+0b99euLu7CwcHB9GkSRMxd+5ceZp3YaKjo8XLL78s6tSpI2xtbUXNmjXFs88+K/bv35/vay5duiRfz4EDB8wec/v2bTF27Fjh5+cnbG1thbe3t3jqqafEihUr5GMM0+DXr19vUVuFKHgavPHPhuH3zfHjx0WHDh2Evb29qFu3rvj888/NtrWw74UQ+mUhPvroI9GkSRNhZ2cnPDw8RFhYmDhx4oRJ+wr72cnIyBBvvfWWCAoKEi4uLsLJyUkEBQWJ//3vfxZ/Haj4JCHK0X+viSqoPn36FGsKMhGVrW7duuHu3btmu+GoauMYIKIiyn3bikuXLuHXX39Ft27dlGkQEREVGccAERVR/fr1MXz4cNSvXx/Xrl3D0qVLYWdnhylTpijdNCIishADEFER9ejRAz/88APi4uKg0WjQoUMHfPDBB3kW9iMiovKLY4CIiIioyuEYICIiIqpyGICIiIioyuEYIDN0Oh1u3boFFxeXIi2xT0RERMoRQiA5ORm+vr55FubMjQHIjFu3bsHPz0/pZhAREVExXL9+HbVr1y7wGAYgMwxLnl+/fh2urq4Kt4aIiIgskZSUBD8/P5MbHeeHAcgMQ7eXq6srAxAREVEFY8nwFQ6CJiIioiqHAYiIiIiqHAYgIiIiqnI4BoiIiEqdTqdDZmam0s2gSsbW1hZqtbpUzsUAREREpSozMxPR0dHQ6XRKN4UqIXd3d3h7e5d4nT4GICIiKjVCCMTGxkKtVsPPz6/QxeiILCWEQFpaGuLj4wEAPj4+JTofAxAREZWa7OxspKWlwdfXF46Ojko3hyoZBwcHAEB8fDw8PT1L1B3GaE5ERKVGq9UCAOzs7BRuCVVWhmCdlZVVovMwABERUanjfRSprJTWzxa7wKxIqxM4Gn0f8cnp8HSxR3v/6lCr+EuCiIjI2hiArGTHuVjM3noesYnp8jYfN3vMDA9Ej+YlG8hFRETlT7169TBhwgRMmDDBouP37NmD7t2748GDB3B3dy/TthG7wKxix7lYjPnupEn4AYC4xHSM+e4kdpyLVahlRETlk1YncPjKPWw5dROHr9yDVifK7L0kSSrwMWvWrGKd99ixYxg9erTFx3fs2BGxsbFwc3Mr1vtZas+ePZAkCQkJCWX6PuUdK0BlTKsTmL31PMz90xUAJACzt57H04He7A4jIoL1K+axsY/+E7pu3TrMmDEDFy9elLc5OzvLnwshoNVqYWNT+J9PDw+PIrXDzs4O3t7eRXoNFR8rQGXsaPT9PJUfYwJAbGI6jkbft16jiIjKKSUq5t7e3vLDzc0NkiTJz//++2+4uLhg+/btaNOmDTQaDQ4cOIArV66gd+/e8PLygrOzM9q1a4ddu3aZnLdevXpYtGiR/FySJHz11Vfo27cvHB0dERAQgJ9//lnen7syExERAXd3d+zcuRNNmzaFs7MzevToYRLYsrOz8frrr8Pd3R01atTA22+/jWHDhqFPnz7F/no8ePAAQ4cORbVq1eDo6IiwsDBcunRJ3n/t2jWEh4ejWrVqcHJyQrNmzfDrr7/Krx0yZAg8PDzg4OCAgIAArFq1qthtKUsMQGUsPjn/8FOc44iIKhIhBNIysy16JKdnYebPf+VbMQeAWT+fR3J6lkXnE6L0us3eeecdfPjhh7hw4QJatmyJlJQU9OzZE7t378aff/6JHj16IDw8HDExMQWeZ/bs2RgwYADOnDmDnj17YsiQIbh/P///AKelpeHjjz/Gt99+i3379iEmJgaTJ0+W98+fPx/ff/89Vq1ahYMHDyIpKQmbN28u0bUOHz4cx48fx88//4zDhw9DCIGePXvK087Hjh2LjIwM7Nu3D2fPnsX8+fPlKtn06dNx/vx5bN++HRcuXMDSpUtRs2bNErWnrLALrIx5utiX6nFERBXJwywtAmfsLJVzCQBxSeloMes3i44/PycUjnal82duzpw5ePrpp+Xn1atXR1BQkPz8vffew6ZNm/Dzzz9j3Lhx+Z5n+PDhGDx4MADggw8+wGeffYajR4+iR48eZo/PysrCsmXL0KBBAwDAuHHjMGfOHHn/kiVLMHXqVPTt2xcA8Pnnn8vVmOK4dOkSfv75Zxw8eBAdO3YEAHz//ffw8/PD5s2b8fzzzyMmJgb9+/dHixYtAAD169eXXx8TE4PWrVujbdu2APRVsPKKFaAy1t6/Onzc7JHf6B4J+r7t9v7VrdksIiIqAsMfdIOUlBRMnjwZTZs2hbu7O5ydnXHhwoVCK0AtW7aUP3dycoKrq6t8awdzHB0d5fAD6G//YDg+MTERt2/fRvv27eX9arUabdq0KdK1Gbtw4QJsbGwQHBwsb6tRowYaN26MCxcuAABef/11vP/+++jUqRNmzpyJM2fOyMeOGTMGa9euRatWrTBlyhQcOnSo2G0pa6wAlTG1SsLM8ECM+e5knn2GUDQzPJADoImoUnKwVeP8nFCLjj0afR/DVx0r9LiIEe0s+k+jg23p3DUc0IcVY5MnT0ZkZCQ+/vhjNGzYEA4ODnjuueeQmZlZ4HlsbW1NnkuSVOBNY80dX5pde8UxatQohIaGYtu2bfjtt98wb948LFy4EOPHj0dYWBiuXbuGX3/9FZGRkXjqqacwduxYfPzxx4q22RxWgKygR3MfLH3hMbg7mP4ge7vZY+kLj3EdICKqtCRJgqOdjUWPzgEeFlXMOwd4WHS+slyN+uDBgxg+fDj69u2LFi1awNvbG1evXi2z9zPHzc0NXl5eOHbsUWjUarU4eTLvf7gt1bRpU2RnZ+PIkSPytnv37uHixYsIDAyUt/n5+eHVV1/Fxo0bMWnSJHz55ZfyPg8PDwwbNgzfffcdFi1ahBUrVhS7PWWJFSAr6dHcB9lagXE//IkGNZ3wft8WXAmaiMiIccVcAkwGQ5e3inlAQAA2btyI8PBwSJKE6dOnF1jJKSvjx4/HvHnz0LBhQzRp0gRLlizBgwcPLAp/Z8+ehYuLi/xckiQEBQWhd+/eePnll7F8+XK4uLjgnXfeQa1atdC7d28AwIQJExAWFoZGjRrhwYMHiIqKQtOmTQEAM2bMQJs2bdCsWTNkZGTgl19+kfeVNwxAVmSfU451drBFhwY1FG4NEVH5Y6iY514HyLucrZz/ySef4KWXXkLHjh1Rs2ZNvP3220hKSrJ6O95++23ExcVh6NChUKvVGD16NEJDQy26S3qXLl1MnqvVamRnZ2PVqlV444038J///AeZmZno0qULfv31V7k7TqvVYuzYsbhx4wZcXV3Ro0cPfPrppwD0axlNnToVV69ehYODAzp37oy1a9eW/oWXAkko3ZlYDiUlJcHNzQ2JiYlwdXUttfPu++cOhn59FE19XLH9jc6ldl4iovIiPT0d0dHR8Pf3h7198We38t6JxaPT6dC0aVMMGDAA7733ntLNKRMF/YwV5e83K0BWZGejH3KVma1VuCVEROWbWiWxUm6Ba9eu4bfffkPXrl2RkZGBzz//HNHR0fi///s/pZtW7nEQtBXZqvVf7iwti25ERFRyKpUKERERaNeuHTp16oSzZ89i165d5XbcTXnCCpAVaeQKkPUHyhERUeXj5+eHgwcPKt2MCokVICuSu8C0DEBERERKYgCyIrkLjBUgIiIiRTEAWZGhApTBChAREZGiGICsyE79aAwQVx8gIiJSDgOQFRkqQABnghERESmJAciKDBUgAMhiNxgREZFiGICsyLgCxKnwRESVS7du3TBhwgT5eb169bBo0aICXyNJEjZv3lzi9y6t81QlDEBWpFZJ8lLunApPRFQ+hIeHo0ePHmb37d+/H5Ik4cyZM0U+77FjxzB69OiSNs/ErFmz0KpVqzzbY2NjERYWVqrvlVtERATc3d3L9D2siQHIymzVOQGIFSAioryi5gF7F5jft3eBfn8pGzlyJCIjI3Hjxo08+1atWoW2bduiZcuWRT6vh4cHHB0dS6OJhfL29oZGo7HKe1UWDEBWJs8EYwWIiCgvlRqImps3BO1doN+uKvwu50X1n//8Bx4eHoiIiDDZnpKSgvXr12PkyJG4d+8eBg8ejFq1asHR0REtWrTADz/8UOB5c3eBXbp0CV26dIG9vT0CAwMRGRmZ5zVvv/02GjVqBEdHR9SvXx/Tp09HVlYWAH0FZvbs2Th9+jQkSYIkSXKbc3eBnT17Fk8++SQcHBxQo0YNjB49GikpKfL+4cOHo0+fPvj444/h4+ODGjVqYOzYsfJ7FUdMTAx69+4NZ2dnuLq6YsCAAbh9+7a8//Tp0+jevTtcXFzg6uqKNm3a4Pjx4wD09zQLDw9HtWrV4OTkhGbNmuHXX38tdlssoWgAmjdvHtq1awcXFxd4enqiT58+uHjxYqGvW79+PZo0aQJ7e3u0aNEizxdJCIEZM2bAx8cHDg4OCAkJwaVLl8rqMorEzkb/j5cVICKqEoQAMlMtf3QYC3R5Sx92fn9fv+339/XPu7yl32/puSxcbsTGxgZDhw5FRESEyRIl69evh1arxeDBg5Geno42bdpg27ZtOHfuHEaPHo0XX3wRR48eteg9dDod+vXrBzs7Oxw5cgTLli3D22+/nec4FxcXRERE4Pz581i8eDG+/PJLfPrppwCAgQMHYtKkSWjWrBliY2MRGxuLgQMH5jlHamoqQkNDUa1aNRw7dgzr16/Hrl27MG7cOJPjoqKicOXKFURFRWH16tWIiIjIEwItpdPp0Lt3b9y/fx979+5FZGQk/v33X5P2DRkyBLVr18axY8dw4sQJvPPOO7C1tQUAjB07FhkZGdi3bx/Onj2L+fPnw9nZuVhtsZSi9wLbu3cvxo4di3bt2iE7Oxv//e9/8cwzz+D8+fNwcnIy+5pDhw5h8ODBmDdvHv7zn/9gzZo16NOnD06ePInmzZsDABYsWIDPPvsMq1evhr+/P6ZPn47Q0FCcP38e9vb21rzEPOzYBUZEVUlWGvCBb/Feu+8j/SO/54X57y3AzvzfktxeeuklfPTRR9i7dy+6desGQN/91b9/f7i5ucHNzQ2TJ0+Wjx8/fjx27tyJH3/8Ee3bty/0/Lt27cLff/+NnTt3wtdX//X44IMP8ozbeffdd+XP69Wrh8mTJ2Pt2rWYMmUKHBwc4OzsDBsbG3h7e+f7XmvWrEF6ejq++eYb+W/p559/jvDwcMyfPx9eXl4AgGrVquHzzz+HWq1GkyZN0KtXL+zevRsvv/yyRV8zY7t378bZs2cRHR0NPz8/AMA333yDZs2a4dixY2jXrh1iYmLw1ltvoUmTJgCAgIAA+fUxMTHo378/WrRoAQCoX79+kdtQVIpWgHbs2IHhw4ejWbNmCAoKQkREBGJiYnDixIl8X7N48WL06NEDb731Fpo2bYr33nsPjz32GD7//HMA+urPokWL8O6776J3795o2bIlvvnmG9y6datcjJA3zATjNHgiovKjSZMm6NixI77++msAwOXLl7F//36MHDkSAKDVavHee++hRYsWqF69OpydnbFz507ExMRYdP4LFy7Az89PDj8A0KFDhzzHrVu3Dp06dYK3tzecnZ3x7rvvWvwexu8VFBRkUkjo1KkTdDqdSS9Ls2bNoFY/6lL08fFBfHx8kd7L+D39/Pzk8AMAgYGBcHd3x4ULFwAAEydOxKhRoxASEoIPP/wQV65ckY99/fXX8f7776NTp06YOXNmsQadF1W5uht8YmIiAKB69er5HnP48GFMnDjRZFtoaKgcbqKjoxEXF4eQkBB5v5ubG4KDg3H48GEMGjQozzkzMjKQkZEhP09KSirJZRTIjneEJ6KqxNZRX4kpqgOf6qs9ajtAm6nv/nrizaK/dxGMHDkS48ePxxdffIFVq1ahQYMG6Nq1KwDgo48+wuLFi7Fo0SK0aNECTk5OmDBhAjIzM4vWpgIcPnwYQ4YMwezZsxEaGgo3NzesXbsWCxcuLLX3MGbofjKQJAk6Xdn9bZo1axb+7//+D9u2bcP27dsxc+ZMrF27Fn379sWoUaMQGhqKbdu24bfffsO8efOwcOFCjB8/vszaU24GQet0OkyYMAGdOnWSu7LMiYuLk8t3Bl5eXoiLi5P3G7bld0xu8+bNk0ucbm5uJgm2tPF+YERUpUiSvhuqKI/DX+jDT/dpwPQ7+o/7PtJvL8p5JKlITR0wYABUKhXWrFmDb775Bi+99BKknHMcPHgQvXv3xgsvvICgoCDUr18f//zzj8Xnbtq0Ka5fv47Y2Fh52x9//GFyzKFDh1C3bl1MmzYNbdu2RUBAAK5du2ZyjJ2dHbRabaHvdfr0aaSmpsrbDh48CJVKhcaNG1vc5qIwXN/169flbefPn0dCQgICAwPlbY0aNcKbb76J3377Df369cOqVavkfX5+fnj11VexceNGTJo0CV9++WWZtNWg3ASgsWPH4ty5c1i7dq3V33vq1KlITEyUH8bfwNLGO8ITERXAMNur+zSg6xT9tq5T9M/NzQ4rRc7Ozhg4cCCmTp2K2NhYDB8+XN4XEBCAyMhIHDp0CBcuXMArr7xiMsOpMCEhIWjUqBGGDRuG06dPY//+/Zg2bZrJMQEBAYiJicHatWtx5coVfPbZZ9i0aZPJMfXq1UN0dDROnTqFu3fvmvReGAwZMgT29vYYNmwYzp07h6ioKIwfPx4vvvhinuJAUWm1Wpw6dcrkceHCBYSEhKBFixYYMmQITp48iaNHj2Lo0KHo2rUr2rZti4cPH2LcuHHYs2cPrl27hoMHD+LYsWNo2rQpAGDChAnYuXMnoqOjcfLkSURFRcn7ykq5CEDjxo3DL7/8gqioKNSuXbvAY729vfP80N2+fVseEGb4WNAxuWk0Gri6upo8ygqnwRMRFUCnNQ0/BoYQpCu4+lFSI0eOxIMHDxAaGmoyXufdd9/FY489htDQUHTr1g3e3t7o06ePxedVqVTYtGkTHj58iPbt22PUqFGYO3euyTHPPvss3nzzTYwbNw6tWrXCoUOHMH36dJNj+vfvjx49eqB79+7w8PAwOxXf0dERO3fuxP3799GuXTs899xzeOqpp+SxsiWRkpKC1q1bmzzCw8MhSRK2bNmCatWqoUuXLggJCUH9+vWxbt06AIBarca9e/cwdOhQNGrUCAMGDEBYWBhmz54NQB+sxo4di6ZNm6JHjx5o1KgR/ve//5W4vQWRhIK3JRdCYPz48di0aRP27NljMiI8PwMHDkRaWhq2bt0qb+vYsSNatmyJZcuWQQgBX19fTJ48GZMmTQKgH9Pj6emJiIgIs2OAcktKSoKbmxsSExNLPQy9uPII9l+6i08GBKHfYwWHPSKiiiY9PR3R0dHw9/dXfNYtVU4F/YwV5e+3ooOgx44dizVr1mDLli1wcXGRx+i4ubnBwcEBADB06FDUqlUL8+bpV/9844030LVrVyxcuBC9evXC2rVrcfz4caxYsQKAfhDXhAkT8P777yMgIECeBu/r61uktF5W5AoQu8CIiIgUo2gAWrp0KQDIay4YrFq1Su57jYmJgUr1qKeuY8eOWLNmDd59913897//RUBAADZv3mwycHrKlClITU3F6NGjkZCQgCeeeAI7duwoF/8b4TR4IiIi5SkagCzpfduzZ0+ebc8//zyef/75fF8jSRLmzJmDOXPmlKR5ZUKeBcYKEBERkWLKxSDoqoSDoImIiJTHAGRltoYusGzFxp4TEZU5BefXUCVXWj9bDEBW9qgCVLZTOYmIlGC4tUJprpBMZCwtLQ1A3pWsi6pc3QqjKtDwVhhEVInZ2NjA0dERd+7cga2trckkFqKSEEIgLS0N8fHxcHd3N7mPWXEwAFmZLafBE1ElJkkSfHx8EB0dnec2DkSlwd3dPd+FjYuCAcjK5Juhatk/TkSVk52dHQICAtgNRqXO1ta2xJUfAwYgK+Pd4ImoKlCpVOVi7TWi/LBz1so4DZ6IiEh5DEBW9mgaPAMQERGRUhiArEzDChAREZHiGICsjGOAiIiIlMcAZGWcBk9ERKQ8BiArezQNngGIiIhIKQxAVsYuMCIiIuUxAFkZp8ETEREpjwHIyuxsJABAFgMQERGRYhiArMzOcKdkdoEREREphgHIyjgGiIiISHkMQFZmq9Z3gTEAERERKYcByMo4DZ6IiEh5DEBWZhyAhBAKt4aIiKhqYgCyMk3OIGghgGwdAxAREZESGICszDZnGjzAqfBERERKYQCyMsNCiAAHQhMRESmFAcjKbNQqqHKKQAxAREREymAAUoDhjvAZDEBERESKYABSgGEmGMcAERERKYMBSAEargVERESkKAYgBch3hGcXGBERkSIYgBRgyy4wIiIiRTEAWUPUPGDvAvmpnfEg6L0L9PuJiIjIahiArEGlBqLmyiHIMAja59Rn+u0qtZKtIyIiqnJslG5AldB1iv5j1Fzg4QPUlDphvHoT/M9uALpPe7SfiIiIrIIByFq6TgGuHgD++B9WYSlUtgIXm76Oxgw/REREVscuMGtqGAIAUEEgQ9jgfKNXFG4QERFR1aRoANq3bx/Cw8Ph6+sLSZKwefPmAo8fPnw4JEnK82jWrJl8zKxZs/Lsb9KkSRlfiYWu7gcA6CBBI2Wj4YX/KdwgIiKiqknRAJSamoqgoCB88cUXFh2/ePFixMbGyo/r16+jevXqeP75502Oa9asmclxBw4cKIvmF83eBcCl3wAAZxw7YmHWc2jxzxcms8OIiIjIOhQdAxQWFoawsDCLj3dzc4Obm5v8fPPmzXjw4AFGjBhhcpyNjQ28vb1LrZ0ltneBfgB0k3Dg762wk7KwRNsP3Rp7ok3UXP0xHAtERERkNRV6DNDKlSsREhKCunXrmmy/dOkSfH19Ub9+fQwZMgQxMTEKtTCHTquf7dWsDwDADlkAgKN1Rum367QKNo6IiKjqqbCzwG7duoXt27djzZo1JtuDg4MRERGBxo0bIzY2FrNnz0bnzp1x7tw5uLi4mD1XRkYGMjIy5OdJSUml29juU/UfL2wFANgKfQDKzNYBIaz8EBERWVuFDUCrV6+Gu7s7+vTpY7LduEutZcuWCA4ORt26dfHjjz9i5MiRZs81b948zJ49uyybq2djD+BRAOKtMIiIiJRRIbvAhBD4+uuv8eKLL8LOzq7AY93d3dGoUSNcvnw532OmTp2KxMRE+XH9+vXSbrKeWt9W25wuMN4NnoiISBkVMgDt3bsXly9fzreiYywlJQVXrlyBj49PvsdoNBq4urqaPMqEjUb/wbgLjIiIiKxO0QCUkpKCU6dO4dSpUwCA6OhonDp1Sh60PHXqVAwdOjTP61auXIng4GA0b948z77Jkydj7969uHr1Kg4dOoS+fftCrVZj8ODBZXotFsmpANmITACsABERESlF0TFAx48fR/fu3eXnEydOBAAMGzYMERERiI2NzTODKzExET/99BMWL15s9pw3btzA4MGDce/ePXh4eOCJJ57AH3/8AQ8Pj7K7EEsZKkC6nADEChAREZEiFA1A3bp1gxAi3/0RERF5trm5uSEtLS3f16xdu7Y0mlY21PoApNaxC4yIiEhJFXIMUIWVUwFSC1aAiIiIlMQAZE2GAKTLBCA4DZ6IiEghDEDWpH40Zd8WWg6CJiIiUggDkDXlVIAA/e0wMtgFRkREpAgGIGtSPwpAGmSxC4yIiEghDEDWpFIBKv3EOztkcRA0ERGRQhiArC3nfmB2UjYDEBERkUIYgKwtZyC0HbvAiIiIFMMAZG05A6E1YAWIiIhIKQxA1mZUAeI0eCIiImUwAFmboQIkcRo8ERGRUhiArC0nAHEMEBERkXIYgKxNbQhAHANERESkFAYgazOqAOkEkM0qEBERkdUxAFmbPAg6GwCQpRVKtoaIiKhKYgCyNkMFSMoCAHaDERERKYAByNpyKkD20AegDK1WydYQERFVSQxA1pZzKwwHlT74sAJERERkfQxA1majrwA5qDgGiIiISCkMQNaWMw3eEIBYASIiIrI+BiBrszEEIHaBERERKYUByNoMg6ANs8C4DhAREZHVMQBZW04FyF5iFxgREZFSGICsTb4Zak4AYgWIiIjI6hiArE2dKwCxAkRERGR1DEDWllMBUmszAQB/3UqEVsep8ERERNbEAGRl524/BACkZ+g/Ltp1CU/M/x07zsUq2SwiIqIqhQHIinaci8XXh28BADQ5t8IAgLjEdIz57iRDEBERkZUwAFmJVicwe+t5ZMAWAKCRHgUgQwfY7K3n2R1GRERkBQxAVnI0+j5iE9PlAGSHbJP9AkBsYjqORt9XoHVERERVCwOQlcQnpwMAMmEDALAz6gIzdxwRERGVHQYgK/F00d8FPjOfClDu44iIiKjsMABZSXv/6vBxs0emMF8BkgD4uNmjvX91BVpHRERUtTAAWYlaJWFmeKBcATIeBC3lfJwZHgi1SjLzaiIiIipNNko3oCrp0dwHzv9pCUSaVoC83ewxMzwQPZr7KNg6IiKiqoMByMqeaFobiASc1Fr984Y1sfql9qz8EBERWZGiXWD79u1DeHg4fH19IUkSNm/eXODxe/bsgSRJeR5xcXEmx33xxReoV68e7O3tERwcjKNHj5bhVRRRzr3AbIS+AuRib8PwQ0REZGWKBqDU1FQEBQXhiy++KNLrLl68iNjYWPnh6ekp71u3bh0mTpyImTNn4uTJkwgKCkJoaCji4+NLu/nFY7gXmMiGBB3Ss7QKN4iIiKjqUbQLLCwsDGFhYUV+naenJ9zd3c3u++STT/Dyyy9jxIgRAIBly5Zh27Zt+Prrr/HOO++UpLmlQ20nf2qHbKRn8W7wRERE1lYhZ4G1atUKPj4+ePrpp3Hw4EF5e2ZmJk6cOIGQkBB5m0qlQkhICA4fPqxEU/PKqQAB+vuBZWSzAkRERGRtFSoA+fj4YNmyZfjpp5/w008/wc/PD926dcPJkycBAHfv3oVWq4WXl5fJ67y8vPKMEzKWkZGBpKQkk0eZYQWIiIhIcRVqFljjxo3RuHFj+XnHjh1x5coVfPrpp/j222+Lfd558+Zh9uzZpdHEwkmSfiC0NgN2yEI6K0BERERWV6EqQOa0b98ely9fBgDUrFkTarUat2/fNjnm9u3b8Pb2zvccU6dORWJiovy4fv16mbbZ0A1mJ2UhgxUgIiIiq6vwAejUqVPw8dEvIGhnZ4c2bdpg9+7d8n6dTofdu3ejQ4cO+Z5Do9HA1dXV5FGmcrrB9F1grAARERFZm6JdYCkpKXL1BgCio6Nx6tQpVK9eHXXq1MHUqVNx8+ZNfPPNNwCARYsWwd/fH82aNUN6ejq++uor/P777/jtt9/kc0ycOBHDhg1D27Zt0b59eyxatAipqanyrLByIacCpEEWAxAREZECFA1Ax48fR/fu3eXnEydOBAAMGzYMERERiI2NRUxMjLw/MzMTkyZNws2bN+Ho6IiWLVti165dJucYOHAg7ty5gxkzZiAuLg6tWrXCjh078gyMVpRcAcpCRja7wIiIiKxNEkIIpRtR3iQlJcHNzQ2JiYll0x32xePAnQsYnDkNh3XNcHluGGzUFb43koiISFFF+fvNv7pKsNFXgDQ5N0RNZxWIiIjIqhiAlJBzPzDDHeE5DoiIiMi6GICUkDMI2jHnjvAMQERERNbFAKSEnEHQzupsAOBq0ERERFbGAKSEXBUg3g+MiIjIuhiAlGAIQCpDFxgrQERERNbEAKQEda4KEMcAERERWRUDkBJypsE7SDljgNgFRkREZFUMQErIqQA5qDgImoiISAkMQEqwMQ1AHARNRERkXQxASsiZBq+RWAEiIiJSAgOQEmzsAQD2cgBiBYiIiMiaGICUYGO4GzwrQEREREpgAFJCziBojcR7gRERESmBAUgJuStAHARNRERkVQxASpDvBp8JAMhgFxgREZFVMQApwcYQgDgNnoiISAkMQErICUC2wjAGiBUgIiIia2IAUkJOF5iN0HeBcRA0ERGRdTEAKSFnELSN4CwwIiIiJTAAKSFPBYhdYERERNbEAKSEnDFAap2+AsRB0ERERNbFAKSEnACk0rECREREpAQGICWoDRWgnADEChAREZFVMQApIWcQtKTjQohERERKYABSQk4FSKXlNHgiIiIlMAApIWcMkCS0UEPLAERERGRlDEBKUNvJn9ohCxnZ7AIjIiKyJgYgJdjYy5/aIRvZOoFsLUMQERGRtTAAKUFtA0j6L70dclaDZhWIiIjIahiAlJIzEFoj6e8Iz3FARERE1lOsAHT9+nXcuHFDfn706FFMmDABK1asKLWGVXo5U+Gd1PrgwwBERERkPcUKQP/3f/+HqKgoAEBcXByefvppHD16FNOmTcOcOXNKtYGVVk4FyMXGUAFiFxgREZG1FCsAnTt3Du3btwcA/Pjjj2jevDkOHTqE77//HhEREaXZvsorZyq8s1offHg/MCIiIuspVgDKysqCRqP/A75r1y48++yzAIAmTZogNja29FpXmRkCkI2hC4wVICIiImspVgBq1qwZli1bhv379yMyMhI9evQAANy6dQs1atQo1QZWWjldYE45ASiDY4CIiIisplgBaP78+Vi+fDm6deuGwYMHIygoCADw888/y11jlti3bx/Cw8Ph6+sLSZKwefPmAo/fuHEjnn76aXh4eMDV1RUdOnTAzp07TY6ZNWsWJEkyeTRp0qTI11jmcgZBO6pyKkDsAiMiIrIam+K8qFu3brh79y6SkpJQrVo1efvo0aPh6Oho8XlSU1MRFBSEl156Cf369Sv0+H379uHpp5/GBx98AHd3d6xatQrh4eE4cuQIWrduLR/XrFkz7Nq1S35uY1OsyyxbORUgRzW7wIiIiKytWMng4cOHEELI4efatWvYtGkTmjZtitDQUIvPExYWhrCwMIuPX7RokcnzDz74AFu2bMHWrVtNApCNjQ28vb0tPq/VRM0DVGqg65RH0+BV+llg9c59Dtx1BrpPVbKFREREVUKxusB69+6Nb775BgCQkJCA4OBgLFy4EH369MHSpUtLtYEF0el0SE5ORvXq1U22X7p0Cb6+vqhfvz6GDBmCmJiYAs+TkZGBpKQkk0eZUKmBqLnA3gVyBchBpcV49UYEXvxcv5+IiIjKXLEC0MmTJ9G5c2cAwIYNG+Dl5YVr167hm2++wWeffVaqDSzIxx9/jJSUFAwYMEDeFhwcjIiICOzYsQNLly5FdHQ0OnfujOTk5HzPM2/ePLi5uckPPz+/smlw1ylA92n6EJRwDQDQ9uEBTLLdgJMNXtPvJyIiojJXrC6wtLQ0uLi4AAB+++039OvXDyqVCo8//jiuXbtWqg3Mz5o1azB79mxs2bIFnp6e8nbjLrWWLVsiODgYdevWxY8//oiRI0eaPdfUqVMxceJE+XlSUlLZhiBAH4IAtEg9jIVZz8HRbxQeK5t3JCIiolyKVQFq2LAhNm/ejOvXr2Pnzp145plnAADx8fFwdXUt1Qaas3btWowaNQo//vgjQkJCCjzW3d0djRo1wuXLl/M9RqPRwNXV1eRRprpOkW+GqoMKS7T9eCsMIiIiKypWAJoxYwYmT56MevXqoX379ujQoQMAfTXIeDByWfjhhx8wYsQI/PDDD+jVq1ehx6ekpODKlSvw8fEp03YVyd4FgNDP+lJBh/HqjZwGT0REZEXF6gJ77rnn8MQTTyA2NlZeAwgAnnrqKfTt29fi86SkpJhUZqKjo3Hq1ClUr14dderUwdSpU3Hz5k15wPWaNWswbNgwLF68GMHBwYiLiwMAODg4wM3NDQAwefJkhIeHo27durh16xZmzpwJtVqNwYMHF+dSS9/eBfrur7odgWuHcNO5GSalbMDvMdUAfKx064iIiKqEYlWAAMDb2xutW7fGrVu35DvDt2/fvkiLDh4/fhytW7eWq0YTJ05E69atMWPGDABAbGysyQyuFStWIDs7G2PHjoWPj4/8eOONN+Rjbty4gcGDB6Nx48YYMGAAatSogT/++AMeHh7FvdTSYwg/3acBDfVddwlODbAw6zk8Gfulfj8RERGVuWJVgHQ6Hd5//30sXLgQKSkpAAAXFxdMmjQJ06ZNg0plWa7q1q0bhBD57s99Y9U9e/YUes61a9da9N6K0Gn14afrFODIcgCAvUjHEu0gNPVxRU8du8GIiIisoVgBaNq0aVi5ciU+/PBDdOrUCQBw4MABzJo1C+np6Zg7d26pNrLSMF7k0M4JAKDRPQQAbHV/AT27t1GiVURERFVOsQLQ6tWr8dVXX8l3gQf0U85r1aqF1157jQHIEjkByC4nAHEWGBERkfUUawzQ/fv3zY71adKkCe7fv1/iRlUJds76D9o0ALwXGBERkTUVKwAFBQXh888/z7P9888/R8uWLUvcqCohpwJkawhAnAZPRERkNcXqAluwYAF69eqFXbt2yWsAHT58GNevX8evv/5aqg2stHICkI1W3wWWwQoQERGR1RSrAtS1a1f8888/6Nu3LxISEpCQkIB+/frhr7/+wrffflvabayccrrA1NmpAFgBIiIisqZiVYAAwNfXN89g59OnT2PlypVYsWJFiRtW6ckBKA2AYAWIiIjIioq9ECKVUE4XmCR0sEcmZ4ERERFZEQOQUmwd5U+dkM4AREREZEUMQEpRqQBbfRXIUUpHeja7wIiIiKylSGOA+vXrV+D+hISEkrSl6rFzArJS4YQMaHUC2VodbNTMpERERGWtSAHIcMf1gvYPHTq0RA2qUuycgFTAEekAgPRsHZwZgIiIiMpckQLQqlWryqodVVPOTDAnKR0Q+tthOGuKPTGPiIiILMRyg5JyZoK5SBkAgENX7kKrE0q2iIiIqEpgAFLQnUx9tcde6FeDfv2HU3hi/u/YcS5WyWYRERFVegxACtlxLhbHb2UCABxzKkAAEJeYjjHfnWQIIiIiKkMMQArQ6gRmbz2PFGEPAHDGQ3mfoQNs9tbz7A4jIiIqIwxACjgafR+xielIhT4AOUrpJvsFgNjEdByNvq9A64iIiCo/BiAFxCfrA09aTgByQkaBxxEREVHpYgBSgKeLPvik5nSBGdYByu84IiIiKl0MQApo718dPm72SIMGQM46QEYkAD5u9mjvX12B1hEREVV+DEAKUKskzAwPlLvAjCtAUs7HmeGBUKskM68mIiKikuKywwrp0dwH3k80BY6YjgHydrPHzPBA9Gjuo2DriIiIKjcGIAW1alAbOALUdtIBCcCTTTzw5dB2rPwQERGVMXaBKclwKwy1vgLkYGfD8ENERGQFDEBKyrkZqp02DQCQ9DBLydYQERFVGQxASsoJQLaGAJSerWRriIiIqgwGICXldIHZZKcBEEhOZwWIiIjIGhiAlJQTgCShhQZZSHrIChAREZE1MAApKScAAfq1gFgBIiIisg4GICWp1ICNAwDAScpARrYOGdlahRtFRERU+TEAKS2nCmRYDTqZA6GJiIjKHAOQ0nICkIedvvuLU+GJiIjKHgOQ0nKmwtfICUCsABEREZU9BiClaXICkG1OBYgDoYmIiMocA5DScrrAqttmAmAFiIiIyBoUDUD79u1DeHg4fH19IUkSNm/eXOhr9uzZg8ceewwajQYNGzZEREREnmO++OIL1KtXD/b29ggODsbRo0dLv/GlJScAuav1AYhjgIiIiMqeogEoNTUVQUFB+OKLLyw6Pjo6Gr169UL37t1x6tQpTJgwAaNGjcLOnTvlY9atW4eJEydi5syZOHnyJIKCghAaGor4+PiyuoySyRkD5KpmBYiIiMhabJR887CwMISFhVl8/LJly+Dv74+FCxcCAJo2bYoDBw7g008/RWhoKADgk08+wcsvv4wRI0bIr9m2bRu+/vprvPPOO6V/ESWVUwFyzbkjPMcAERERlb0KNQbo8OHDCAkJMdkWGhqKw4cPAwAyMzNx4sQJk2NUKhVCQkLkY8zJyMhAUlKSycNqcgKQs6QPQKwAERERlb0KFYDi4uLg5eVlss3LywtJSUl4+PAh7t69C61Wa/aYuLi4fM87b948uLm5yQ8/P78yab9ZOV1gTpJ+IUSOASIiIip7FSoAlZWpU6ciMTFRfly/ft16b55rJegkVoCIiIjKnKJjgIrK29sbt2/fNtl2+/ZtuLq6wsHBAWq1Gmq12uwx3t7e+Z5Xo9FAo9GUSZsLlROA7IUhALECREREVNYqVAWoQ4cO2L17t8m2yMhIdOjQAQBgZ2eHNm3amByj0+mwe/du+ZhyJ6cLTKN7CIBjgIiIiKxB0QCUkpKCU6dO4dSpUwD009xPnTqFmJgYAPquqaFDh8rHv/rqq/j3338xZcoU/P333/jf//6HH3/8EW+++aZ8zMSJE/Hll19i9erVuHDhAsaMGYPU1FR5Vli5kxOA7LRpADgGiIiIyBoU7QI7fvw4unfvLj+fOHEiAGDYsGGIiIhAbGysHIYAwN/fH9u2bcObb76JxYsXo3bt2vjqq6/kKfAAMHDgQNy5cwczZsxAXFwcWrVqhR07duQZGF1u5HSB2eYEoGR2gREREZU5SQghlG5EeZOUlAQ3NzckJibC1dW1bN/s5kngy+7QutRCgzsfQZKAK3N7QqWSyvZ9iYiIKpmi/P2uUGOAKqWcLjBVVioAQAggNZPjgIiIiMoSA5DScrrApMxU2Kn13w5OhSciIipbDEBKywlA0GWhur2+N5LjgIiIiMoWA5DSDAEIgKe9FgCQ9JAVICIiorLEAKSUqHnA3gWA2hZQ6xdh9LDTV35qnFik309ERERlokKtBF2pqNRA1Fz95xpnIC0DNeyyMV69DQ3ObQC6T1O2fURERJUYA5BSuk7Rf4yaC9i7AQDCH25BZ9tf8GfD19DasJ+IiIhKHQOQkoxDEIDOSb9gYdZz0Pi+hNYKNouIiKiy4xggpXWdAkC/6KEWaizR9uM0eCIiojLGAKS0vQsA6Ke/q6HFePVGToMnIiIqYwxAStq7QN/9VasNACDWvQ0m2W5AxxtfK9wwIiKiyo0BSCmG8NN9GtCoBwAg3bUeFmY9h/D7q3IqQ0RERFQWGICUotPqw0/XKYCTBwDAOfsBlmj74XvHF/T7iYiIqExwFphSuk999LmzJwDAPuMeAGBxVl/Ur9Ma7XUCat4VnoiIqNSxAlQeOHsBAFLu3QQAxCdnYPCXf+CJ+b9jx7lYJVtGRERUKTEAlQN7b+pngVUXiTDMCAOAuMR0jPnuJEMQERFRKWMAUphWJzBz9x0AgEbKggseyvsMUWj21vPQ6oSZVxMREVFxMAAp7Gj0fVxNEkgSDgAADynBZL8AEJuYjqPR963fOCIiokqKAUhh8cnpAIC7Qn8/sJpILPA4IiIiKjkGIIV5utgDAO4iJwBJ5gOQ4TgiIiIqOQYghbX3rw4fN3u5AuSRKwBJAHzc7NHev7oCrSMiIqqcGIAUplZJmBke+KgLzCgAGVYAmhkeyPWAiIiIShEDUDnQo7kPOrRsCgDwQIK83dvNHktfeAw9mvso1DIiIqLKiStBlxMBDRoA54HHvbTATcCvmgP2vNWdlR8iIqIywApQeeGkvx2GtzoZAHA/NRPMPkRERGWDAai8yLkdhiZdvyhiaqYWD9KylGwRERFRpcUAVF446+8IL6XegZeLHQDg+v00JVtERERUaTEAlRc5XWDQZqKRu/62F9cfMAARERGVBQag8sLWHtDop8I3ddYHn+v3Hxb0CiIiIiomBqDyJKcbrIGjPvjcYAWIiIioTDAAlSc53WB+dikAgOsPWAEiIiIqCwxA5YmzYSq8fjXoGxwETUREVCYYgMqTnABkuCP8jQcPodMJJVtERERUKTEAlSc5XWDO2Q+gVknI1OoQn5yhcKOIiIgqHwag8iSnAqRKjYePmz0ADoQmIiIqC7wXWHkQNQ9QqQHvFvrnqfGo5W6PGw8eInHHXFz3dIRvnzm8LxgREVEpKRcVoC+++AL16tWDvb09goODcfTo0XyP7datGyRJyvPo1auXfMzw4cPz7O/Ro4c1LqV4VGogai7wz04AwMMHcThzIwnj1RvxVNxX+PHELTwx/3fsOBercEOJiIgqB8UD0Lp16zBx4kTMnDkTJ0+eRFBQEEJDQxEfH2/2+I0bNyI2NlZ+nDt3Dmq1Gs8//7zJcT169DA57ocffrDG5RRP1ylA92nAiVUAAFXaHYzSrcck2w1YmPUclmj7IS4xHWO+O8kQREREVAoUD0CffPIJXn75ZYwYMQKBgYFYtmwZHB0d8fXXX5s9vnr16vD29pYfkZGRcHR0zBOANBqNyXHVqlWzxuUUX9cp0HWZAgDQSNkm4QcADHPBZm89Dy1nhhEREZWIogEoMzMTJ06cQEhIiLxNpVIhJCQEhw8ftugcK1euxKBBg+Dk5GSyfc+ePfD09ETjxo0xZswY3Lt3L99zZGRkICkpyeShhCN1X4XIyTZZQi2HHwMBIDYxHUej71u/cURERJWIogHo7t270Gq18PLyMtnu5eWFuLi4Ql9/9OhRnDt3DqNGjTLZ3qNHD3zzzTfYvXs35s+fj7179yIsLAxardbseebNmwc3Nzf54efnV/yLKgH3Y59CyhnnbCtpMV690exx8cnpVmwVERFR5VOhZ4GtXLkSLVq0QPv27U22Dxo0SP68RYsWaNmyJRo0aIA9e/bgqaeeynOeqVOnYuLEifLzpKQk64egvQvQ9O8l+FPbAK3VV3BU2wiTbDcAQJ5KkKeLvXXbRkREVMkoWgGqWbMm1Go1bt++bbL99u3b8Pb2LvC1qampWLt2LUaOHFno+9SvXx81a9bE5cuXze7XaDRwdXU1eVjV3gVA1Fzouv0XkRp9d2AKHLEw6zlMst0gV4IkAD5u9mjvX9267SMiIqpkFA1AdnZ2aNOmDXbv3i1v0+l02L17Nzp06FDga9evX4+MjAy88MILhb7PjRs3cO/ePfj4+JS4zWVCpwW6T4Oq29vo2qkzAKCx6gaWaPthYdZzUEs6GFYAmhkeyPWAiIiISkjxLrCJEydi2LBhaNu2Ldq3b49FixYhNTUVI0aMAAAMHToUtWrVwrx580xet3LlSvTp0wc1atQw2Z6SkoLZs2ejf//+8Pb2xpUrVzBlyhQ0bNgQoaGhVruuIuk+Vf40OLgTsBeoJd2FM9Lk7i8fN3vMDA9Ej+blNMQRERFVIIoHoIEDB+LOnTuYMWMG4uLi0KpVK+zYsUMeGB0TEwOVyrRQdfHiRRw4cAC//fZbnvOp1WqcOXMGq1evRkJCAnx9ffHMM8/gvffeg0ajsco1lYhjdcDFB0iOxbfPuuK5rVpohcB3I4PRwNNZ6dYRERFVCpIQgovK5JKUlAQ3NzckJiZafzwQAHzTB/g3Cgj/DH2PNMSfMQn4dGAQ+raubf22EBERVRBF+fut+EKIZIZnoP7jnb/R2k+/gOOfMQnKtYeIiKiSYQAqjzyb6D/Gn8djdd0BAPsu3cGWUzdx+Mo9rgRNRERUQoqPASIzDBWg+L+R+DALAHD1bhreWHsKAAdEExERlRQrQOWRR2P9x5Q4fLwp7y1BeGNUIiKikmEAKo80LhBu+pWoA6QbeXbzxqhEREQlwwBU3kTNA/YuQIJzQwBAI9WjADRevRETbPS3x+CNUYmIiIqPAai8Uan1t8V4mAgAaJRTARqv3ohJthugFabfMt4YlYiIqOg4CLq86ToFAFAjai4AoLHquhx+FmY9xxujEhERlQIGoPKo6xTokmKhOvE12kt/I9j27zzhRwLgzRujEhERFQu7wMopVdh8CACSBGQKdZ7wA/DGqERERMXFAFReHVwkBx07SYvx6o3yLi83eyx94TGuA0RERFRMDEDl0d4FQNRcoMGTAABRvSEm2W7AZM1mAMAnzwcx/BAREZUAxwCVN4bw030aEPAMcOV3SCm3ga5TMW7vPGSodVh/whd3UjLg6aIfA8RuMCIioqJhACpvdFp9+Ok6Rf+5vTuQngA0fAq7L96B+sZ9bPrzFjb9eQsAb4tBRERUHJIQgksJ55KUlAQ3NzckJibC1dVV2casewG4sBX/NJuAZ060z7PbUPvhmCAiIqrqivL3m2OAyjv/rgCApPO7zO7mbTGIiIiKjgGoPIuaB9y7DABoobsIDTLlXbwtBhERUfExAJVnKjVwZBmybJygkbLQRvUPAN4Wg4iIqKQ4CLo8y7kthm3ObTE6qc6hjfQPb4tBRERUQgxA5V3XKdDFnobq718wRv0zVBLMhh9vVw1vi0FERGQhdoFVAKpnl0AAUElAVq7bYhikZ+sQeT7O+o0jIiKqgBiAKoJjX8nT3W1z3RbDIDEtC2O+O4kd52Kt2zYiIqIKiAGovDOsDO3TCgCQDRUm2W4wCUHj1Rvxhs0GCAD/3XQWmdk6ZdpKRERUQTAAlWfGt8Vo0gsAYAMd9mubyyEo94yw+6lZeHzeblaCiIiICsBB0OWZ8W0xANz6+wh8Y3ehpepffJLVHx1Uf6Gj+kKeQdH3UzMx5ruT+OL/WqOakwbxyem8bxgREZER3grDjHJ1Kwwjf/xzC0HfB8FBykSWUMNW0pqdEWagkgDjxaEN9w17OtAbR6PvMxgREVGlUpS/36wAVSDtGvrgB5ueeEG7GbaSFpnCJt/wA5iGHwCIS0zHq9+dhLujLRLSsuTtvKEqERFVNRwDVIGoVRJ6usfIz+2k7DyDoQ23xzDHkIeMww+gD0acQUZERFUJK0AVyd4FqH7vpPxUKyRMsn0UeAwrRBeVgP6u8rO3nsfTgd7sDiMiokqPAaiiMJ4RBgBRc6GWBOJ1bnIIKmg8UGGMb6jaoUGNUmo0ERFR+cQAVFHkmhGGzFTg4CJ4qhIBAIe0TU3Cz3j1RqglHRZlF60iFHk+jgGIiIgqPY4Bqii6T30UfgDg6dmA9Ojbt1XXUf48v7vFW+Lrg1c5FoiIiCo9VoAqqr0LAKGDfvSOwFzblbgqvNFWupjv3eItwbFARERUFbACVBEZjwea+QCZ9jWgArDGdq7Z8FPY7DBjxmOBiIiIKisGoIrGOPx0nQJIEnY+vQtCAJIECAGs1XaXDy9ud1h8cnqebVqdwOEr97Dl1E0cvnIP2twLDREREVUQ7AKraHIPhgYQFPONHH4kCYiym4jHM7/ACPWOYneHebrYmzzfcS4Ws7eeR2zio2DEBRSJiKiiKhcVoC+++AL16tWDvb09goODcfTo0XyPjYiIgCRJJg97e9M/1kIIzJgxAz4+PnBwcEBISAguXbpU1pdhHbkHQ+9dgDqnP8UK9SB0zfwUmUINZ1UGzmpGFbs7rJqjDbK1OrnS8+uZWIz57qRJ+AEsX0CRlSMiIipvFK8ArVu3DhMnTsSyZcsQHByMRYsWITQ0FBcvXoSnp6fZ17i6uuLixYvyc0kyHay7YMECfPbZZ1i9ejX8/f0xffp0hIaG4vz583nCUoVm1B1Wp8aLuP7dSfTPnI2f7d6VK0LVpWSMV2/EEm0/uTvMsFhiflPlH6Rl48WvH4VQlfRoFWljhm3/3XQWTzbxglol5bnHWOT5OFaOiIio3FH8ZqjBwcFo164dPv/8cwCATqeDn58fxo8fj3feeSfP8REREZgwYQISEhLMnk8IAV9fX0yaNAmTJ08GACQmJsLLywsREREYNGhQoW0qrzdDzSNqHqBSyxWhHediEbNpFkZr18rdYVpIUEPgmNQc7cQ5uSL0lsMWjBXrSrR4ojFnjQ1s1JLJbTZy33PMwBBXl77wGEMQERGVmqL8/Va0CywzMxMnTpxASEiIvE2lUiEkJASHDx/O93UpKSmoW7cu/Pz80Lt3b/z111/yvujoaMTFxZmc083NDcHBwfmeMyMjA0lJSSaPCiFXd1iPe99itHYtYoLexG899iDb1hnqnDpNO3EOGY4+6NnCB/uCj2GsWIfsrv/F9/b6QFiUmWLmpGRk5wk75sIP8KhyNHvreXaHERGRIhQNQHfv3oVWq4WXl5fJdi8vL8TFxZl9TePGjfH1119jy5Yt+O6776DT6dCxY0fcuHEDAOTXFeWc8+bNg5ubm/zw8/Mr6aVZn3F3WN9ZCO3QGjZTruBRvQXQpMWi6d9LUOf0p0D3aThWZxTup2aWaOHE4uJ0eyIiUlK5GARdFB06dMDQoUPRqlUrdO3aFRs3boSHhweWL19e7HNOnToViYmJ8uP69eul2GIrMTM7DIc+AyBMVoyW/RuF+OR0k3FBhnFCJakEFdXBy3c4OJqIiKxO0UHQNWvWhFqtxu3bt0223759G97e3hadw9bWFq1bt8bly5cBQH7d7du34ePzaHzJ7du30apVK7Pn0Gg00Gg0xbiCcqT7VNPnudcLiggHru57tP/aIYRfaw6VrQ4HtYFFGiRdmj6PuiJ/nntwtFYn8gyq5urURERUGhQNQHZ2dmjTpg12796NPn36ANAPgt69ezfGjRtn0Tm0Wi3Onj2Lnj17AgD8/f3h7e2N3bt3y4EnKSkJR44cwZgxY8riMsqf3OFn7wJ9+PHvAkTvA9QaQJsBFXTQAeikPo/v8T46qc8rGobiEtPx6ncn8WZIABIfZmHzqVu4n5op7zcEpKcDvQsMRiUJTqUZuhjgiIjKL8WnwU+cOBHDhg1D27Zt0b59eyxatAipqakYMWIEAGDo0KGoVasW5s2bBwCYM2cOHn/8cTRs2BAJCQn46KOPcO3aNYwaNQqAfkr8hAkT8P777yMgIECeBu/r6yuHrErPuDvMXBi6vBu4/geAR32gndTn8UDnqGgYMnSAfbrL/JpNhoCUe3aZceXI0gUbzYWT0pyyb64d1Z1s0bdVLYQEejMMEREpTPEANHDgQNy5cwczZsxAXFwcWrVqhR07dsiDmGNiYqBSPRrD8uDBA7z88suIi4tDtWrV0KZNGxw6dAiBgYHyMVOmTEFqaipGjx6NhIQEPPHEE9ixY0flWgOoIMbdYebGBl3/Q78tPQk4vETeXE2VBkAfhhJ1DiZhyHiMkHEYsiZDQMo9u8ywIOPoLv5YsS86z5pFhv2Gaffmwkl+U/Zzv9aSqs6Oc/qFI3O3435qFlYevIqVB69yLSQiIoUpvg5QeVRh1gEqKnPVoKi5gKQGhBYCxnPGHrkvnPFt9tPIhhqTbDfgoDYQQ7LeBWBaDbJGN1lBDIs/5qeaow1efLwePvv9ctHOC8DbzR7TewXivW0FV4i0OoEn5v+eZ9Vsc+cETNdCYpcZEVHJFOXvNwOQGZU2ABkvnJg7DC1qCSRck8OQDuanCB7UBqKT+jwO65pCJyS5OgTApDKkZBCyJkM8+eL/WqOakwYHL98xGdhd2Gu93exx4O0nuWK2GeUlEJaXdhBR4RiASqjSBiCD/CpBhkHS7nWBhGvQChXUkk5eVRoAYnXVcFIXgF42+ltlHNQGQoJAR/UFLMx6Dh1Uf8mfG7rMykuFqCypJKC4s/jfDGmERbv+ydNllrtKVNQ/xEU5vrz9kS8vN98tL+0gIsswAJVQpQ9ABVWCVofrQ1BOGLoBT9RGPHRCgkoq+EfFUB0CkCcMVdVgZAl3B1skPDS/anZB3W8FDaouyh/u8vZHPr8xVNa+hUp5aUdlVN4CN1UeDEAlVOkDkDELwlCCdwe4xx1GkkdbuN45XuDp0jXVcSStNrqqz8jb8gtGh7RNIaDvRjukbYrDumYA9N1nhuoTA5Jlcs+Es/QPd1n9kS/sD1x++zOzdXh83m6T5Q9yt8vQbVgafzDza0dhY7lKux2VVVnPtiTKjQGohKpUADJWUBjau0BfGbq63/Q1OWOG8qMT+u6hq/DB4ewmGGwTJe+7pvNAXdUdHNQG4g9dICbZ6legjtHVRB3VXVaOimH8kw3w/ZHr+QYIAPB21eDgO08BQKEDtqs72eKPqSGws7F80fjCKkr57X82yAfrT9zA/VTz1TBjP7z8ODo0qGFxm4raTjcHOwz+8g+rtKOyKspsS1bVqLQwAJVQlQ1AxnLdad4kEF3drw9DwKNxQzIJgMh3RpmBIRjd07mghioZx7SN0E79j8kxlnSp5a4cGQcj4yoSmQpr7o22davhvW0XCj22upMdPujb3KLZavlVlAz+09IH287E5rvfUkM71EVYcx+Tik1RulQKa2dYc29sP2f+3oHGFg9qhd6tahXzKoqvvHchFfb1zU9xArclyvvXi0oPA1AJMQDlYhx+gLxByL8LUK+zfjtgWhVS2QC6bIvexniwtSEgXZDqY3dmC4yz3SIfZxyMjD837lIzriKxclRyEvT/OweQ7wKPTzbxwqT1pxGXVPASAKWpupMtWvu548/riSZVL+M2QQLupmTIf/iAwitflhrbrQE6NqiZ5z1y/3E1/gNc00lT6PHmGM4ReT4u31XSy0P1xNKlIPLjYm+DAW1ql9qCoVV9UdLSCH8VKUAyAJUQA1AuxtUgw+fAo5ljdTrqt0XNlWeQyYyDknEwyrX2kHH4KYghGJ3T1sMBXXO8avuLvM/QpWb4CBQ8GFst6dBO+hvHRJM8wYghyZQEwM3RFolpWSWu3ijJx80eg9r55bvaeGm9h/EtW8wFFnPHmwsvBYUeY6XVhVQaf+gOX7lnUfehJUoa7CypRFm6UnxxZ1Bacq6yChhmuyEdbDGiUz2MezLAovco6iQJpcMSA1AJMQBZIL/xQoD+83qd9eHIUBXK3VVmFIwEJEgQEFBBgk6/X1IBQmdxcwzByBCkYnQ1cVrXEOE2j34R5+5Se1x1Xr7lh2Eqf34Dszupz0ErVAxLFZy+g9Y675HfeBdzxwN5lzsoLPSYO4+5gdmW/kEqzmxAc+f+5cwtvLH2lEVttuSaAPPBzpLgUZxFSUtzBqUl1aeyGhReWPhzd7TFh/1aFPgeRZ0kUR5mlDIAlRADUBGZqxAZByPjMJQ7GBkqRhZWjgwhqbAxRrkZjk9z8cdd95aoc13fpZZ77FF+A7ONt+e37pGhoqSWdDiobQ6A45LIMgUtd1BU47o3RKeGNYv0x7U4swHz+2NX2hU242AHwOJuwKJUooy//mPXmP86CABvhgSgXk0n1HTS4NjV+1i0O+91Gr5m+d2ax1hZDQovSvjLbzX6mk6aAru0cwfuwn6GDIvFlnVliAGohBiASklhwcgQevy76Es3hhlmxtWigipHkgpSTpVIKySoJSF/zF0RsoTh2CQ4whVpuKD1Q1P1dXl/tEsb+CefkJ/nV1Ey7oLLb1yScfcbALOfs9JEJVFYBeqNpxqivX8NxCel471tFwrsXvNy1WDhgFbyuKUHqZkFBgVnjQ1SMiwb+2epsObeOBJ93+JuwC2nbha5EuVib4Pk9NJpd0kWRzWo5miDzwY9hvtpmUUKDUUJfz4FrEZviR9efhzt/asXGrhyfz2K2hVnKQagEmIAKmOGMKTTmh9bZJhuX1DlKM/ss0eBxBBAsoUKNlJON1oBg7EtryY96kAxBKxonSf+1AWgn81BkzYYGNqS6lgbTmk3AOTtfgOQ5/MYXU1cF55yV5xxt5yAVGBwMnxuXInqpD5nUpXiOktUEqXxx72sGGaSnbj2oNTGIpUX5u49aK4bsKjhb8JTAVi8+1Kxuodf6lQPTwd6F/trbUlXXFEwAJUQA5DCitClluZYC45pNwHkX4UBkP9gbLUdoM35H6XKFtBloSQjRYzDlDzAO+djqlMd7Ev0QpjNMQCm3W/5zWzLLyAdk5rDy9UedRKP53uM8dfA+Jz5rbNkripV2iFqgs0GaIUKS7T98uwzd44aTnao7+GEY1cfFPalJ5K52NugX2tfbPzzVqlVdMoD4664xIdZeboBDeOLfN0dLFriIvd5i2tEx7pYdeha4QcW8P6ltQYUA1AJMQCVU+aCkU6LS3fSEHUxHs0zT6Oj+gIOagOhsZHQVvxVeOXI+LnJGkc5vxLym9Kf896PBnAXbUxSbibByUy3XbTOC/6q2wCA89o6CFTHAABSXOrDOflfAMAf2iZ4XP03gLyVqNzPc2/LrypV2iHKcOxBbaAcoo6JJmgvXciz4KWznQTfPnPwz4/TWaEiKiJrVulKY3KBTymtrM4AVEIMQBWPVidwa/MMpGQKJLR7E8HXlkOltil4MLYkmV/HKL+B2fmFpfxCkjyTzbgOJCByPpblxNCCKlHAoy68f7S1cFIEYJDNHgD5V6LMPc+9zZIQZXxMgs4R7qq0PPs7qv5CB/UF/NN0PJL//h1txF95ZuQZV6UMIQooXpdgUc7BsVmlrzx3p5F1lcbK6gxAJcQAVMnk16UWNQ+IOWR+HaPcA7ONtxuHpYIqSsbPDa8vtPsN0Ek2UAl9iDIex2SITJIhykhSkZYKsITpYpSPboBrCEyGj8b7/9HWwnlRF31sDgGAyeDxv7R10Uz9KET+qW2A1uor8grghvOd19bBQdEML9tsl4813IMOyD+YGc/Os1GrEIxzOILmyNbqLK5mGbYfk5pDJ4BgnFN8bFbu1xm6DSfYbChWmCuPoS337KBvDkdj+7nbirWHlFcaK6szAJUQA1AVVNjA7H/36D83Dkv5VZSMK0fGlaKCKkr5fH6nZjA87h4xbatxcMrvc6NAJVeijNZWMgQrQ+Apymw569DXrK7ofHBZ1EKo+tFNeK/qPFFPFS8/v2XfEL7pl/VP6nYCrukHpBt/7QqrZhU0Bsvc5/E1gpGl06HWg2N44NkB/8QnFxicjAOaJd2KxpU0w3IMh7RNoVarCzyHcSA0fu+iLN1QlhU24/C1zukFfNNgDwI8HIHuU4s1a4ssY2+jghACGdry/eeeFaBygAGIClRYRUmnBep30x9rmNFmXFHK3f1mOM6SzwGLglNhlalLga8j4cLvaCfOmVSZdJItVMJQiVJDlVOterS8wKOqkKFaYeha0wl9glLlHGf43PALxnjFb3mZAqOlDMqSudusGG8zXoTT+HPj6zVZqNPAKNQah5JonSf8c0La39raaKLWzwBMrtEKLvdOyS8vqFvR8PEImiMY5+T9uUNx7irbEW1jBKsvmj2/JUs3WBKozH1uSYUtwMMJ7vFHEBP0JvwSjkG6dkD/nwSdFgkX9+LvWw+sEsQq4jkq85pi+S3iWRwMQCXEAESlxpLuNyDv54awZByc/LsAD6496ooDCg5LxhWnfMKRcTdTQccBef+YXgp8HQEPTwHR+6CTVFDlBJkMYQONpO/C06lsocqpRAlJDUlooYMEFQR09bpANXwrsDq80FulmASqnOCiFRIkCVBV0JtzmKu+GcLZLV01bNJ1xjCHg3DOumf6Olg+4N60y1L/ee6q2iWdLwJUt+TX/KOthUbqm/Lzi9paaJzz/KK2NhrnhLnzWj8E5nR1HtU2QnujBUWNB+Sbe57feLob8ERt6INjWQUx42pcSc5RGu0oyjmKu6ZY7iB2SNscAuUrzOmECs+29ELDgR+gpBiASogBiBRX3OBk/LlxJerfPXmrUgUNCjccZxSijMOSrl4XqK6aD0omFYrclajcA8wN+wtY8NK4QmX43GSNJwu6AY3HVcnVrJwgBsBs+NKfw3hQez7HSDl3Lhc6CEmlnzwIXd4xW/qDUFpyByjDb/Ly1ZVZANfagGcT4PIu/XPfx4BbJ+XduYPZOW1dNM+pdN10bIJaafpAdULbEG3U+i7Q3EHssLYpOqgfTQc/pA1Ex5wgclZbDy3UVwEAp7X+CFJHAwCOawPQVv1oheeCqmj3qj+GGvdP5nnv3O+b+xzGVTrj5TDyWxoDyP9eh4WtKVYew9zCLH3VapLtBhyTmqOdOKf/N991CkqKAaiEGICoUitKuCrlEGV2rJTxwPIiLHgpK0aXoMWVL0u7GAHLxmYZfa6FBDUerV4OAFpJDbV4tLyCPH/PZAmGR8HOXCDMFDawy6nAZQk1bCVDN6Zpl6XIeQcVhNkKW35dmrmPBfIPXeVtbFlR2yN/rUy6S4t4G55ijrMzN2vTcI5buuq4KrzlMGdcpTMOcCe1DfCY+op8zjNaf7TMCXqAaag07ko1DpXGn5t7bpjYAJh29xq3KXdV8bS2PoLU/yLRsz3c4o+WWvgBGIBKjAGIqBDFDVE6LdB9qn5pAuOB5UCBC17q3OpAlahf+8gkvBiHKKB4XYJFOUcpj83KE8TMrUVVyMxDSwd0f5L1HIb53kCNO3+YVLCMuyzNBSqtyhbqnMBlfGx+nxteq4UKasOYKZPKnCHMmS4PkZtxUAMMlTTzjMeZFYXJ+LSSnKOYVbgKW70rTaUYfgAGoBJjACJSUD4LXprMyDOuShmHqOJ0CRblHFYam2V2Lap8ZhSadE2614Uq4RqOSc2Rmf1oKYC/7FpitHZt3vcBcM/jcX0oylFQhc2SGXK525TnPS1Zld2wPb/9FnxuGJdmPD4NaltAm2X5+czMoMzvHMYh0LgKZ67L1ZIgafw+uauFhnMJqPQrYghDl6uQK1ZA3nBlEjeNJiQAjypMJq+T8oZD46qY8bHCcA6jwFrQxAjJ8H2eeR+liQGohBiAiMgsa43Nqtvp0euuHdRvz5ktZUmY0/l1wM2Eh7C/dQTpvsGoVd0Zqj0f5NtlaQhOQOEVttzhys5GpR/DYUmFrThraJl7XszZj2V5joICYYxbW9RJfLSUg3HoNBlTZBxG86kWPnT2g0PK9XzDYWGVufyqf/kHsbzB1JJJD8afmxxjeK2hHawAlS8MQERkdcaBKre9Cx51H5b03IUt3VBIdSxPuHJ3gOr6YcvOUZQ1tIrbTangOYzDocn9CM2cQ1evC5LSs+AedxgJ3h3gam+rn1iQXzssXFMsv3CVpx2wsKJXSCDMd9JDQeuZ5Z78wDFA5QcDEBFRGSjFIFamXZ3FPIdxOLRTCbgEhkAlSaXTjiKsKWYcruSKEWASSgxLDhR0w+U2davrQ1lph8ru0/Qfo+aWeghiACohBiAiIipXitv9mk+Yu3/2N1y4nZrvuj1ezbujoadL2YRK48kQhvF9JalwGmEAKiEGICIiqux2nIvF7K3nEZuYLm/zcbPHzPBA9Gjuo2DLiq8of79trNQmIiIiKkd6NPfB04HeOBp9H/HJ6fB0sUd7/+olvh1FRcEAREREVEWpVVKJb0BaUamUbgARERGRtTEAERERUZXDAERERERVDgMQERERVTkMQERERFTlMAARERFRlcMARERERFUOAxARERFVOQxAREREVOVwJWgzDLdHS0pKUrglREREZCnD321LbnPKAGRGcnIyAMDPz0/hlhAREVFRJScnw83NrcBjeDd4M3Q6HW7dugUXFxdIUslvCpeUlAQ/Pz9cv369Ut5dvrJfH8BrrCwq+zVW9usDeI2VQVlenxACycnJ8PX1hUpV8CgfVoDMUKlUqF27dqmf19XVtVL+MBtU9usDeI2VRWW/xsp+fQCvsTIoq+srrPJjwEHQREREVOUwABEREVGVwwBkBRqNBjNnzoRGo1G6KWWisl8fwGusLCr7NVb26wN4jZVBebk+DoImIiKiKocVICIiIqpyGICIiIioymEAIiIioiqHAYiIiIiqHAagMvbFF1+gXr16sLe3R3BwMI4ePap0k4pt3rx5aNeuHVxcXODp6Yk+ffrg4sWLJsekp6dj7NixqFGjBpydndG/f3/cvn1boRaXzIcffghJkjBhwgR5W2W4vps3b+KFF15AjRo14ODggBYtWuD48ePyfiEEZsyYAR8fHzg4OCAkJASXLl1SsMVFo9VqMX36dPj7+8PBwQENGjTAe++9Z3JvoIp2jfv27UN4eDh8fX0hSRI2b95sst+S67l//z6GDBkCV1dXuLu7Y+TIkUhJSbHiVeSvoOvLysrC22+/jRYtWsDJyQm+vr4YOnQobt26ZXKO8nx9QOHfQ2OvvvoqJEnCokWLTLZXhmu8cOECnn32Wbi5ucHJyQnt2rVDTEyMvN+av2MZgMrQunXrMHHiRMycORMnT55EUFAQQkNDER8fr3TTimXv3r0YO3Ys/vjjD0RGRiIrKwvPPPMMUlNT5WPefPNNbN26FevXr8fevXtx69Yt9OvXT8FWF8+xY8ewfPlytGzZ0mR7Rb++Bw8eoFOnTrC1tcX27dtx/vx5LFy4ENWqVZOPWbBgAT777DMsW7YMR44cgZOTE0JDQ5Genq5gyy03f/58LF26FJ9//jkuXLiA+fPnY8GCBViyZIl8TEW7xtTUVAQFBeGLL74wu9+S6xkyZAj++usvREZG4pdffsG+ffswevRoa11CgQq6vrS0NJw8eRLTp0/HyZMnsXHjRly8eBHPPvusyXHl+fqAwr+HBps2bcIff/wBX1/fPPsq+jVeuXIFTzzxBJo0aYI9e/bgzJkzmD59Ouzt7eVjrPo7VlCZad++vRg7dqz8XKvVCl9fXzFv3jwFW1V64uPjBQCxd+9eIYQQCQkJwtbWVqxfv14+5sKFCwKAOHz4sFLNLLLk5GQREBAgIiMjRdeuXcUbb7whhKgc1/f222+LJ554It/9Op1OeHt7i48++kjelpCQIDQajfjhhx+s0cQS69Wrl3jppZdMtvXr108MGTJECFHxrxGA2LRpk/zckus5f/68ACCOHTsmH7N9+3YhSZK4efOm1dpuidzXZ87Ro0cFAHHt2jUhRMW6PiHyv8YbN26IWrVqiXPnzom6deuKTz/9VN5XGa5x4MCB4oUXXsj3Ndb+HcsKUBnJzMzEiRMnEBISIm9TqVQICQnB4cOHFWxZ6UlMTAQAVK9eHQBw4sQJZGVlmVxzkyZNUKdOnQp1zWPHjkWvXr1MrgOoHNf3888/o23btnj++efh6emJ1q1b48svv5T3R0dHIy4uzuQa3dzcEBwcXGGusWPHjti9ezf++ecfAMDp06dx4MABhIWFAagc12jMkus5fPgw3N3d0bZtW/mYkJAQqFQqHDlyxOptLqnExERIkgR3d3cAleP6dDodXnzxRbz11lto1qxZnv0V/Rp1Oh22bduGRo0aITQ0FJ6enggODjbpJrP271gGoDJy9+5daLVaeHl5mWz38vJCXFycQq0qPTqdDhMmTECnTp3QvHlzAEBcXBzs7OzkX0oGFema165di5MnT2LevHl59lWG6/v333+xdOlSBAQEYOfOnRgzZgxef/11rF69GgDk66jIP7fvvPMOBg0ahCZNmsDW1hatW7fGhAkTMGTIEACV4xqNWXI9cXFx8PT0NNlvY2OD6tWrV7hrTk9Px9tvv43BgwfLN9KsDNc3f/582NjY4PXXXze7v6JfY3x8PFJSUvDhhx+iR48e+O2339C3b1/069cPe/fuBWD937G8GzwVy9ixY3Hu3DkcOHBA6aaUmuvXr+ONN95AZGSkSZ90ZaLT6dC2bVt88MEHAIDWrVvj3LlzWLZsGYYNG6Zw60rHjz/+iO+//x5r1qxBs2bNcOrUKUyYMAG+vr6V5hqrqqysLAwYMABCCCxdulTp5pSaEydOYPHixTh58iQkSVK6OWVCp9MBAHr37o0333wTANCqVSscOnQIy5YtQ9euXa3eJlaAykjNmjWhVqvzjF6/ffs2vL29FWpV6Rg3bhx++eUXREVFoXbt2vJ2b29vZGZmIiEhweT4inLNJ06cQHx8PB577DHY2NjAxsYGe/fuxWeffQYbGxt4eXlV6OsDAB8fHwQGBppsa9q0qTwLw3AdFfnn9q233pKrQC1atMCLL76IN998U67qVYZrNGbJ9Xh7e+eZfJGdnY379+9XmGs2hJ9r164hMjJSrv4AFf/69u/fj/j4eNSpU0f+3XPt2jVMmjQJ9erVA1Dxr7FmzZqwsbEp9PePNX/HMgCVETs7O7Rp0wa7d++Wt+l0OuzevRsdOnRQsGXFJ4TAuHHjsGnTJvz+++/w9/c32d+mTRvY2tqaXPPFixcRExNTIa75qaeewtmzZ3Hq1Cn50bZtWwwZMkT+vCJfHwB06tQpz9IF//zzD+rWrQsA8Pf3h7e3t8k1JiUl4ciRIxXmGtPS0qBSmf5qU6vV8v9AK8M1GrPkejp06ICEhAScOHFCPub333+HTqdDcHCw1dtcVIbwc+nSJezatQs1atQw2V/Rr+/FF1/EmTNnTH73+Pr64q233sLOnTsBVPxrtLOzQ7t27Qr8/WP1vyGlPqyaZGvXrhUajUZERESI8+fPi9GjRwt3d3cRFxendNOKZcyYMcLNzU3s2bNHxMbGyo+0tDT5mFdffVXUqVNH/P777+L48eOiQ4cOokOHDgq2umSMZ4EJUfGv7+jRo8LGxkbMnTtXXLp0SXz//ffC0dFRfPfdd/IxH374oXB3dxdbtmwRZ86cEb179xb+/v7i4cOHCrbccsOGDRO1atUSv/zyi4iOjhYbN24UNWvWFFOmTJGPqWjXmJycLP7880/x559/CgDik08+EX/++ac8C8qS6+nRo4do3bq1OHLkiDhw4IAICAgQgwcPVuqSTBR0fZmZmeLZZ58VtWvXFqdOnTL53ZORkSGfozxfnxCFfw9zyz0LTIiKf40bN24Utra2YsWKFeLSpUtiyZIlQq1Wi/3798vnsObvWAagMrZkyRJRp04dYWdnJ9q3by/++OMPpZtUbADMPlatWiUf8/DhQ/Haa6+JatWqCUdHR9G3b18RGxurXKNLKHcAqgzXt3XrVtG8eXOh0WhEkyZNxIoVK0z263Q6MX36dOHl5SU0Go146qmnxMWLFxVqbdElJSWJN954Q9SpU0fY29uL+vXri2nTppn8saxo1xgVFWX2396wYcOEEJZdz71798TgwYOFs7OzcHV1FSNGjBDJyckKXE1eBV1fdHR0vr97oqKi5HOU5+sTovDvYW7mAlBluMaVK1eKhg0bCnt7exEUFCQ2b95scg5r/o6VhDBaHpWIiIioCuAYICIiIqpyGICIiIioymEAIiIioiqHAYiIiIiqHAYgIiIiqnIYgIiIiKjKYQAiIiKiKocBiIjIApIkYfPmzUo3g4hKCQMQEZV7w4cPhyRJeR49evRQumlEVEHZKN0AIiJL9OjRA6tWrTLZptFoFGoNEVV0rAARUYWg0Wjg7e1t8qhWrRoAfffU0qVLERYWBgcHB9SvXx8bNmwwef3Zs2fx5JNPwsHBATVq1MDo0aORkpJicszXX3+NZs2aQaPRwMfHB+PGjTPZf/fuXfTt2xeOjo4ICAjAzz//XLYXTURlhgGIiCqF6dOno3///jh9+jSGDBmCQYMG4cKFCwCA1NRUhIaGolq1ajh27BjWr1+PXbt2mQScpUuXYuzYsRg9ejTOnj2Ln3/+GQ0bNjR5j9mzZ2PAgAE4c+YMevbsiSFDhuD+/ftWvU4iKiVlcotVIqJSNGzYMKFWq4WTk5PJY+7cuUIIIQCIV1991eQ1wcHBYsyYMUIIIVasWCGqVasmUlJS5P3btm0TKpVKxMXFCSGE8PX1FdOmTcu3DQDEu+++Kz9PSUkRAMT27dtL7TqJyHo4BoiIKoTu3btj6dKlJtuqV68uf96hQweTfR06dMCpU6cAABcuXEBQUBCcnJzk/Z06dYJOp8PFixchSRJu3bqFp556qsA2tGzZUv7cyckJrq6uiI+PL+4lEZGCGICIqEJwcnLK0yVVWhwcHCw6ztbW1uS5JEnQ6XRl0SQiKmMcA0RElcIff/yR53nTpk0BAE2bNsXp06eRmpoq7z948CBUKhUaN24MFxcX1KtXD7t377Zqm4lIOawAEVGFkJGRgbi4OJNtNjY2qFmzJgBg/fr1aNu2LZ544gl8//33OHr0KFauXAkAGDJkCGbOnIlhw4Zh1qxZuHPnDsaPH48XX3wRXl5eAIBZs2bh1VdfhaenJ8LCwpCcnIyDBw9i/Pjx1r1QIrIKBiAiqhB27NgBHx8fk22NGzfG33//DUA/Q2vt2rV47bXX4OPjgx9++AGBgYEAAEdHR+zcuRNvvPEG2rVrB0dHR/Tv3x+ffPKJfK5hw4YhPT0dn376KSZPnoyaNWviueees94FEpFVSUIIoXQjiIhKQpIkbNq0CX369FG6KURUQXAMEBEREVU5DEBERERU5XAMEBFVeOzJJ6KiYgWIiIiIqhwGICIiIqpyGICIiIioymEAIiIioiqHAYiIiIiqHAYgIiIiqnIYgIiIiKjKYQAiIiKiKocBiIiIiKqc/wcOG1Pp7h/8WAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 1000\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.SGD(linear_classifier.parameters(), lr=0.2, momentum=0.9, nesterov=True)\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: get_lr(step, total_steps, 0.2, 1e-3))\n",
    "\n",
    "linear_classifier.train()\n",
    "\n",
    "# Early stopping setup\n",
    "early_stopping_patience = 15\n",
    "min_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "epsilon = 1e-3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Data preperation setup\n",
    "n_classes = 10\n",
    "samples_per_class = 250\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Create a new subset every epoch\n",
    "    indices = np.hstack([np.random.choice(np.where(np.array(train_dataset.targets) == i)[0], samples_per_class, replace=False) for i in range(n_classes)])\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    linear_classifier.train()\n",
    "\n",
    "    # Training phase\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = linear_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    linear_classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = linear_classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Average validation loss for this epoch\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check based on validation loss\n",
    "    if val_loss < min_val_loss - epsilon:\n",
    "        min_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    elif epoch > 20:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, marker='x', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(linear_classifier.state_dict(), 'linear_classifier_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the SSL + Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 88.95 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "linear_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = linear_classifier(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
