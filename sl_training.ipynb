{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Validation Loss: 1.6285\n",
      "Epoch [2/100], Validation Loss: 1.4025\n",
      "Epoch [3/100], Validation Loss: 1.3024\n",
      "Epoch [4/100], Validation Loss: 1.2039\n",
      "Epoch [5/100], Validation Loss: 1.1163\n",
      "Epoch [6/100], Validation Loss: 1.0680\n",
      "Epoch [7/100], Validation Loss: 1.0556\n",
      "Epoch [8/100], Validation Loss: 1.0111\n",
      "Epoch [9/100], Validation Loss: 0.9604\n",
      "Epoch [10/100], Validation Loss: 0.9391\n",
      "Epoch [11/100], Validation Loss: 0.9053\n",
      "Epoch [12/100], Validation Loss: 0.8965\n",
      "Epoch [13/100], Validation Loss: 0.8620\n",
      "Epoch [14/100], Validation Loss: 0.8393\n",
      "Epoch [15/100], Validation Loss: 0.8169\n",
      "Epoch [16/100], Validation Loss: 0.8313\n",
      "Epoch [17/100], Validation Loss: 0.8151\n",
      "Epoch [18/100], Validation Loss: 0.8218\n",
      "Epoch [19/100], Validation Loss: 0.7941\n",
      "Epoch [20/100], Validation Loss: 0.7933\n",
      "Epoch [21/100], Validation Loss: 0.7786\n",
      "Epoch [22/100], Validation Loss: 0.7551\n",
      "Epoch [23/100], Validation Loss: 0.7522\n",
      "Epoch [24/100], Validation Loss: 0.7367\n",
      "Epoch [25/100], Validation Loss: 0.7259\n",
      "Epoch [26/100], Validation Loss: 0.7438\n",
      "Epoch [27/100], Validation Loss: 0.7152\n",
      "Epoch [28/100], Validation Loss: 0.7055\n",
      "Epoch [29/100], Validation Loss: 0.6890\n",
      "Epoch [30/100], Validation Loss: 0.7089\n",
      "Epoch [31/100], Validation Loss: 0.6785\n",
      "Epoch [32/100], Validation Loss: 0.6929\n",
      "Epoch [33/100], Validation Loss: 0.6774\n",
      "Epoch [34/100], Validation Loss: 0.6634\n",
      "Epoch [35/100], Validation Loss: 0.6747\n",
      "Epoch [36/100], Validation Loss: 0.6559\n",
      "Epoch [37/100], Validation Loss: 0.6552\n",
      "Epoch [38/100], Validation Loss: 0.6632\n",
      "Epoch [39/100], Validation Loss: 0.6395\n",
      "Epoch [40/100], Validation Loss: 0.6461\n",
      "Epoch [41/100], Validation Loss: 0.6460\n",
      "Epoch [42/100], Validation Loss: 0.6344\n",
      "Epoch [43/100], Validation Loss: 0.6232\n",
      "Epoch [44/100], Validation Loss: 0.6173\n",
      "Epoch [45/100], Validation Loss: 0.6146\n",
      "Epoch [46/100], Validation Loss: 0.6179\n",
      "Epoch [47/100], Validation Loss: 0.6070\n",
      "Epoch [48/100], Validation Loss: 0.6108\n",
      "Epoch [49/100], Validation Loss: 0.6362\n",
      "Epoch [50/100], Validation Loss: 0.5964\n",
      "Epoch [51/100], Validation Loss: 0.6067\n",
      "Epoch [52/100], Validation Loss: 0.5871\n",
      "Epoch [53/100], Validation Loss: 0.5929\n",
      "Epoch [54/100], Validation Loss: 0.5753\n",
      "Epoch [55/100], Validation Loss: 0.5979\n",
      "Epoch [56/100], Validation Loss: 0.5785\n",
      "Epoch [57/100], Validation Loss: 0.5745\n",
      "Epoch [58/100], Validation Loss: 0.5814\n",
      "Epoch [59/100], Validation Loss: 0.5682\n",
      "Epoch [60/100], Validation Loss: 0.5602\n",
      "Epoch [61/100], Validation Loss: 0.5738\n",
      "Epoch [62/100], Validation Loss: 0.5618\n",
      "Epoch [63/100], Validation Loss: 0.5707\n",
      "Epoch [64/100], Validation Loss: 0.5534\n",
      "Epoch [65/100], Validation Loss: 0.5847\n",
      "Epoch [66/100], Validation Loss: 0.5657\n",
      "Epoch [67/100], Validation Loss: 0.5517\n",
      "Epoch [68/100], Validation Loss: 0.5542\n",
      "Epoch [69/100], Validation Loss: 0.5488\n",
      "Epoch [70/100], Validation Loss: 0.5356\n",
      "Epoch [71/100], Validation Loss: 0.5524\n",
      "Epoch [72/100], Validation Loss: 0.5402\n",
      "Epoch [73/100], Validation Loss: 0.5311\n",
      "Epoch [74/100], Validation Loss: 0.5344\n",
      "Epoch [75/100], Validation Loss: 0.5357\n",
      "Epoch [76/100], Validation Loss: 0.5192\n",
      "Epoch [77/100], Validation Loss: 0.5414\n",
      "Epoch [78/100], Validation Loss: 0.5334\n",
      "Epoch [79/100], Validation Loss: 0.5422\n",
      "Epoch [80/100], Validation Loss: 0.5389\n",
      "Epoch [81/100], Validation Loss: 0.5188\n",
      "Early stopping at epoch 81\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "epsilon = 1e-3\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best_loss-epsilon:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the supervised learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
